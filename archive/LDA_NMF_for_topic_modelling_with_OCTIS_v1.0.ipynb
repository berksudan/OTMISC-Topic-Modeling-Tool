{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA/NMF for topic modelling with OCTIS.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "l-K8SZ1orVFS",
        "HiDd1uJzxJbY",
        "khItwfu-x7wb",
        "YmRQU4bE-7Xt",
        "6u4xhVWO6Vrf",
        "QKjlr5x3Qe-s",
        "fQPuG9EP-RT3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tweets preprocessing"
      ],
      "metadata": {
        "id": "l-K8SZ1orVFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing for Crisis dataset v1.0\n",
        "\n",
        "# This notebook is created in Google Colab, please change the paths to your file.\n",
        "# Every preprocessing method is seperate. You can choose the ones that you need. \n",
        "# Write me a note if something goes wrong or you need some new preprocessing methods.\n",
        "\n",
        "# Enjoy!"
      ],
      "metadata": {
        "id": "XGK58hhta7PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7rkLDWfzttO",
        "outputId": "c26a7439-3e59-4031-a5e4-0475374ce7ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6sqi26SsYUM",
        "outputId": "a96ed227-4ade-44e8-f1cd-ecc779a8dff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.1.72)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import unicodedata\n",
        "import re\n",
        "import contractions\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiUzg8JzrZ28",
        "outputId": "d186590b-b08a-404a-caca-e5d865090dc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path1 = '/content/drive/MyDrive/SS_2022_Praktikum/Crisis Dataset/Dataset_12/earthquakes_eyewitness_crowdflower_2000.tsv'\n",
        "path2 = '/content/drive/MyDrive/SS_2022_Praktikum/Crisis Dataset/Dataset_12/floods_eyewitness_crowdflower_2000.tsv'\n",
        "path3 = '/content/drive/MyDrive/SS_2022_Praktikum/Crisis Dataset/Dataset_12/forestfires_eyewitness_crowdflower_2000.tsv'\n",
        "path4 = '/content/drive/MyDrive/SS_2022_Praktikum/Crisis Dataset/Dataset_12/hurricanes_eyewitness_crowdflower_2000.tsv'\n",
        "\n",
        "tweets_df1=pd.read_csv(path1, sep=\"\\t\")\n",
        "tweets_df2=pd.read_csv(path2, sep=\"\\t\")\n",
        "tweets_df3=pd.read_csv(path3, sep=\"\\t\")\n",
        "tweets_df4=pd.read_csv(path4, sep=\"\\t\")\n",
        "\n",
        "tweets_df1 = tweets_df1[['text']]\n",
        "tweets_df2 = tweets_df2[['text']]\n",
        "tweets_df3 = tweets_df3[['text']]\n",
        "tweets_df4 = tweets_df4[['text']]\n",
        "\n",
        "tweets_df1.rename(columns={'text':'Tweets'},inplace=True)\n",
        "tweets_df2.rename(columns={'text':'Tweets'},inplace=True)\n",
        "tweets_df3.rename(columns={'text':'Tweets'},inplace=True)\n",
        "tweets_df4.rename(columns={'text':'Tweets'},inplace=True)\n",
        "\n",
        "# Give the topic\n",
        "tweets_df1.loc[:, 'Topics'] = 'earthquakes'\n",
        "tweets_df2.loc[:, 'Topics'] = 'floods'\n",
        "tweets_df3.loc[:, 'Topics'] = 'forestfires'\n",
        "tweets_df4.loc[:, 'Topics'] = 'hurricanes'\n",
        "\n",
        "# Concatenate\n",
        "tweets_df = tweets_df1\n",
        "tweets_df = tweets_df.append(tweets_df2, ignore_index = True)\n",
        "tweets_df = tweets_df.append(tweets_df3, ignore_index = True)\n",
        "tweets_df = tweets_df.append(tweets_df4, ignore_index = True)\n",
        "\n",
        "# Shuffle\n",
        "# Discuss whether to use this method or not, reason: no seed!\n",
        "tweets_df = tweets_df.sample(frac=1.0).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "UR63O8znsoOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the partition of train, dev and test set, make sure they sum up to 1\n",
        "PT_TRAIN = 0.8\n",
        "PT_DEV = 0.1\n",
        "PT_TEST = 0.1\n",
        "\n",
        "# Define train, dev and test set\n",
        "tweets_df.loc[: tweets_df.shape[0] * PT_TRAIN, 'partition'] = 'train'\n",
        "tweets_df.loc[tweets_df.shape[0] * PT_TRAIN : tweets_df.shape[0] * (PT_TRAIN + PT_DEV), 'partition'] = 'dev'\n",
        "tweets_df.loc[tweets_df.shape[0] * (PT_TRAIN + PT_DEV) : tweets_df.shape[0] - 1, 'partition'] = 'test'\n",
        "\n",
        "# Shuffle again\n",
        "tweets_df = tweets_df.sample(frac=1.0).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "6S4bS4c72vAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See if the partition is correct\n",
        "tweets_df.partition.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wd6gGgLHrIy",
        "outputId": "c49b32ff-2fcb-4af4-8a72-1edd86c39dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train    6400\n",
              "dev       800\n",
              "test      800\n",
              "Name: partition, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the order of the df\n",
        "\n",
        "order = ['Tweets', 'partition', 'Topics']\n",
        "tweets_df = tweets_df[order]"
      ],
      "metadata": {
        "id": "xS-Boo2bQa26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment to see the merged (or unmerged) dataframes\n",
        "tweets_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Dqd3T_BtcXyb",
        "outputId": "d0cef357-ba28-440e-d8b1-048787f33d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 Tweets partition       Topics\n",
              "0     Impact small and not until May -cyclone Debbie...     train   hurricanes\n",
              "1     An Areal Flood WARNING is in effect from Green...     train       floods\n",
              "2      Rheal_talk you're going to appreciate my late...     train       floods\n",
              "3     Was there an earthquake? Lol my cousin asked m...     train  earthquakes\n",
              "4     via *npr: 'Public Calamity' As California Wild...     train  forestfires\n",
              "...                                                 ...       ...          ...\n",
              "7995   BrennanKnighton Oh wow and the roads are star...     train       floods\n",
              "7996  Going out to check on my host's other farm and...      test   hurricanes\n",
              "7997  THIS weekend!  Escape Cyclone Debbie and get o...     train   hurricanes\n",
              "7998  https://t.co/AEMEPfjwWR  by Norton Identity Sa...     train  forestfires\n",
              "7999  Canada wildfire rages on: CNBC's Deirdre Bosa ...     train  forestfires\n",
              "\n",
              "[8000 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f721a0fd-d90a-476c-bae7-c0293627fe55\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweets</th>\n",
              "      <th>partition</th>\n",
              "      <th>Topics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Impact small and not until May -cyclone Debbie...</td>\n",
              "      <td>train</td>\n",
              "      <td>hurricanes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>An Areal Flood WARNING is in effect from Green...</td>\n",
              "      <td>train</td>\n",
              "      <td>floods</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Rheal_talk you're going to appreciate my late...</td>\n",
              "      <td>train</td>\n",
              "      <td>floods</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Was there an earthquake? Lol my cousin asked m...</td>\n",
              "      <td>train</td>\n",
              "      <td>earthquakes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>via *npr: 'Public Calamity' As California Wild...</td>\n",
              "      <td>train</td>\n",
              "      <td>forestfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7995</th>\n",
              "      <td>BrennanKnighton Oh wow and the roads are star...</td>\n",
              "      <td>train</td>\n",
              "      <td>floods</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7996</th>\n",
              "      <td>Going out to check on my host's other farm and...</td>\n",
              "      <td>test</td>\n",
              "      <td>hurricanes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7997</th>\n",
              "      <td>THIS weekend!  Escape Cyclone Debbie and get o...</td>\n",
              "      <td>train</td>\n",
              "      <td>hurricanes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7998</th>\n",
              "      <td>https://t.co/AEMEPfjwWR  by Norton Identity Sa...</td>\n",
              "      <td>train</td>\n",
              "      <td>forestfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7999</th>\n",
              "      <td>Canada wildfire rages on: CNBC's Deirdre Bosa ...</td>\n",
              "      <td>train</td>\n",
              "      <td>forestfires</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8000 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f721a0fd-d90a-476c-bae7-c0293627fe55')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f721a0fd-d90a-476c-bae7-c0293627fe55 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f721a0fd-d90a-476c-bae7-c0293627fe55');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure you run this one before other methods!\n",
        "\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "#testing the function on a single sample for explaination\n",
        "print(to_lowercase('IN CHINESE WE CALL CAPITALIZATION AS BIG WRITTING, IN GERMAN AS WELL.'))\n",
        "\n",
        "#converting every row of the column into lower case \n",
        "tweets_df.Tweets=tweets_df.Tweets.apply(to_lowercase)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w7dSsootJAL",
        "outputId": "3d4f8608-7136-42fd-deb9-0093db3dacd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in chinese we call capitalization as big writting, in german as well.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_accented_chars(text):\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "#testing the function on a single sample for explaination\n",
        "print(standardize_accented_chars('sómě words such as résumé, café, prótest, divorcé, coördinate, exposé, latté.'))\n",
        "\n",
        "#standardizing accented characters for every row\n",
        "tweets_df.Tweets=tweets_df.Tweets.apply(standardize_accented_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSDEBSOvhIvC",
        "outputId": "f0e1470e-e8b9-4f6c-fcd7-7b6ded0e9cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "some words such as resume, cafe, protest, divorce, coordinate, expose, latte.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Not a method, just to check how many tweets contain urls\n",
        "\n",
        "def get_number_of_urls(documents):\n",
        "    print(\"{:.2f}% of documents contain urls\".format(sum\n",
        "(documents.apply(lambda x:x.find('http'))>0)/len\n",
        "(documents)*100))\n",
        "\n",
        "# Passing the 'Tweets' column of the dataframe as the argument\n",
        "print(get_number_of_urls(tweets_df.Tweets)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDI0GypOuaAk",
        "outputId": "8c367c39-a5ef-4ba4-bdba-64c8b0db328d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57.98% of documents contain urls\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_url(text):\n",
        "    return re.sub(r'https?:\\S*', '', text)\n",
        "\n",
        "#testing the function on a single sample for explaination\n",
        "print(remove_url('using https://www.google.com/ as an example'))\n",
        "\n",
        "#removing urls from every row\n",
        "tweets_df.Tweets=tweets_df.Tweets.apply(remove_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CC5YGzeuo6j",
        "outputId": "1ce49c14-ecfe-4ce5-cee9-2311353bce82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using  as an example\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_contractions(text):\n",
        "    expanded_words = [] \n",
        "    for word in text.split():\n",
        "       expanded_words.append(contractions.fix(word)) \n",
        "    return ' '.join(expanded_words)\n",
        "\n",
        "#testing the function on a single sample for explaination\n",
        "print(expand_contractions(\"Don't is the same as do not\"))\n",
        "\n",
        "#expanding contractions for every row\n",
        "tweets_df.Tweets=tweets_df.Tweets.apply(expand_contractions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMXOG8d7uz5B",
        "outputId": "b89a33a3-5d7f-485c-e185-f55bfd9558cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do not is the same as do not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_mentions_and_tags(text):\n",
        "    text = re.sub(r'@\\S*', '', text)\n",
        "    return re.sub(r'#\\S*', '', text)\n",
        "\n",
        "#testing the function on a single sample for explaination\n",
        "print(remove_mentions_and_tags('Some random @abc and #def'))\n",
        "\n",
        "#removing mentions and tags from every row\n",
        "tweets_df.Tweets=tweets_df.Tweets.apply(remove_mentions_and_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt4MYvt6vZ2i",
        "outputId": "ab511e63-34e2-4372-a79c-d815d000243c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some random  and \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def keep_only_alphabet(text):\n",
        "    return re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "\n",
        "#testing the function on a single sample for explaination\n",
        "print(keep_only_alphabet('Just a bit more $$processing required.Just a bit!!!'))\n",
        "\n",
        "#for all the rows\n",
        "tweets_df.Tweets=tweets_df.Tweets.apply(keep_only_alphabet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPNZbplPvugU",
        "outputId": "42f79397-6816-4b85-cce9-374ea1e5553e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Just a bit more   processing required Just a bit   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(text):\n",
        "  \"\"\"\n",
        "  Returns text without stop words\n",
        "  \"\"\"\n",
        "  text = word_tokenize(text)\n",
        "  word_list = []\n",
        "  for word in text:\n",
        "      if word not in stopwords.words('english'):\n",
        "          word_list.append(word)\n",
        "\n",
        "  return ' '.join(word_list)\n",
        "\n",
        "#testing the function on a single sample for explaination\n",
        "print(remove_stop_words('Test this text to see which are stop words.'))\n",
        "\n",
        "#removing stop-words and short words from every row\n",
        "tweets_df.Tweets=tweets_df.Tweets.apply(remove_stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsh2oi6NwK2B",
        "outputId": "85bff80b-9723-46a2-f2e7-b531aa66e30a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test text see stop words .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(text):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  text_str = word_tokenize(text)\n",
        "  new_words = []\n",
        "\n",
        "  for word in text_str:\n",
        "    new_words.append(lemmatizer.lemmatize(word))\n",
        "  return ' '.join(new_words)\n",
        "\n",
        "#testing the function on a single sample for explaination\n",
        "print(lemmatize('apples, bananas and pears are common fruits that are eaten by humans.'))\n",
        "\n",
        "#Performing lemmatization on every row\n",
        "tweets_df.Tweets=tweets_df.Tweets.apply(lemmatize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjqFnTvXxksF",
        "outputId": "e8d7d50a-4896-42bf-bd59-bc67b51d84ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple , banana and pear are common fruit that are eaten by human .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete blank rows to fit OCTIS\n",
        "tweets_df = tweets_df[tweets_df['Tweets'].str.len()>=1]\n",
        "tweets_df.reset_index(inplace=True)\n",
        "\n",
        "# Delete the index column caused by reset_index\n",
        "del tweets_df['index']\n",
        "\n",
        "# (OPTINAL) Delete nonsense sentences(like sentences with less than 3 words)\n",
        "# (Discuss) Do you have a better way to extract tweets with more than 3 words? Not string.\n",
        "# Check some word tokenization methods\n",
        "#tweets_df = tweets_df[tweets_df['Tweets'].str.len()>=20]"
      ],
      "metadata": {
        "id": "jQMVZktFKF3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment this to check how the df looks like after running\n",
        "tweets_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2271
        },
        "id": "oZaYj9MfhVlS",
        "outputId": "e52890fa-4df6-4e02-8236-11a20118e7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 Tweets partition       Topics\n",
              "0     creeping wildfire smoke threat california wine...     train  forestfires\n",
              "1     thought affected cyclone debbie last day corn ...     train   hurricanes\n",
              "2     roshayyred know lmao totally forgot though dam...      test       floods\n",
              "3     keeping aussi peep prayer woth storm still bla...     train   hurricanes\n",
              "4     queenslanders beautiful twitter amp please sta...     train   hurricanes\n",
              "...                                                 ...       ...          ...\n",
              "7984  basement flood lost mine kid personal item alo...     train       floods\n",
              "7985          nwsbrownsville friend sent video get home       dev       floods\n",
              "7986  local agency work together wildfire training c...     train  forestfires\n",
              "7987  national art centre collect donation support v...     train  forestfires\n",
              "7988  shoutout one time thought little brother runni...     train  earthquakes\n",
              "\n",
              "[7989 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c2d80781-7e00-4cfc-b9d8-aaadedd94f7b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweets</th>\n",
              "      <th>partition</th>\n",
              "      <th>Topics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>creeping wildfire smoke threat california wine...</td>\n",
              "      <td>train</td>\n",
              "      <td>forestfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thought affected cyclone debbie last day corn ...</td>\n",
              "      <td>train</td>\n",
              "      <td>hurricanes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>roshayyred know lmao totally forgot though dam...</td>\n",
              "      <td>test</td>\n",
              "      <td>floods</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>keeping aussi peep prayer woth storm still bla...</td>\n",
              "      <td>train</td>\n",
              "      <td>hurricanes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>queenslanders beautiful twitter amp please sta...</td>\n",
              "      <td>train</td>\n",
              "      <td>hurricanes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7984</th>\n",
              "      <td>basement flood lost mine kid personal item alo...</td>\n",
              "      <td>train</td>\n",
              "      <td>floods</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7985</th>\n",
              "      <td>nwsbrownsville friend sent video get home</td>\n",
              "      <td>dev</td>\n",
              "      <td>floods</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7986</th>\n",
              "      <td>local agency work together wildfire training c...</td>\n",
              "      <td>train</td>\n",
              "      <td>forestfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7987</th>\n",
              "      <td>national art centre collect donation support v...</td>\n",
              "      <td>train</td>\n",
              "      <td>forestfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7988</th>\n",
              "      <td>shoutout one time thought little brother runni...</td>\n",
              "      <td>train</td>\n",
              "      <td>earthquakes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7989 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2d80781-7e00-4cfc-b9d8-aaadedd94f7b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c2d80781-7e00-4cfc-b9d8-aaadedd94f7b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c2d80781-7e00-4cfc-b9d8-aaadedd94f7b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O2eDk-xMaO4",
        "outputId": "bf51a861-58fd-45c0-f365-c6078e4f4ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7989"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocabulary.txt\n",
        "\n",
        "def df_to_vocab(df):\n",
        "    word_list = []\n",
        "    for i in range(df.shape[0]):\n",
        "        text = word_tokenize(df.Tweets[i])\n",
        "        for word in text:\n",
        "            if word not in word_list:\n",
        "                word_list.append(word)\n",
        "    return word_list\n",
        "\n",
        "word_list = df_to_vocab(tweets_df)\n",
        "len(word_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsacAIAOKIrR",
        "outputId": "e1e1cd95-9be3-442d-9786-6ff5bf81fb4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9330"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_vocab(word_list):\n",
        "    txt = open(\"/content/drive/MyDrive/SS_2022_Praktikum/Crisis Dataset/Dataset_12/vocabulary.txt\", 'w')\n",
        "    for i in range(len(word_list)):\n",
        "        txt.write(word_list[i])\n",
        "        txt.write('\\r\\n')\n",
        "    txt.close()\n",
        "\n",
        "list_to_vocab(word_list)"
      ],
      "metadata": {
        "id": "WwYi6YQpMaZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df.to_csv('/content/drive/MyDrive/SS_2022_Praktikum/Crisis Dataset/Dataset_12/corpus.tsv',\\\n",
        "                 sep = '\\t', index=False, header = None)"
      ],
      "metadata": {
        "id": "WmDCmmc5yU4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stop here"
      ],
      "metadata": {
        "id": "eVjQopw4A5z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OCTIS Initiation"
      ],
      "metadata": {
        "id": "HiDd1uJzxJbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install octis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I6eJMjcxP1b",
        "outputId": "49caff16-da02-4caf-83cb-176f31c8872e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: octis in /usr/local/lib/python3.7/dist-packages (1.10.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from octis) (3.2.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from octis) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.7/dist-packages (from octis) (1.21.6)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from octis) (2.2.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from octis) (1.11.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from octis) (2.23.0)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from octis) (4.2.0)\n",
            "Requirement already satisfied: tomotopy in /usr/local/lib/python3.7/dist-packages (from octis) (0.12.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (from octis) (2.2.0)\n",
            "Requirement already satisfied: libsvm in /usr/local/lib/python3.7/dist-packages (from octis) (3.23.0.4)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from octis) (1.1.4)\n",
            "Requirement already satisfied: scikit-optimize>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from octis) (0.9.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from octis) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn==0.24.2 in /usr/local/lib/python3.7/dist-packages (from octis) (0.24.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2->octis) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2->octis) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.2->octis) (1.1.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=4.0.0->octis) (6.0.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize>=0.8.1->octis) (21.10.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize>=0.8.1->octis) (6.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->octis) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->octis) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->octis) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->octis) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->octis) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->octis) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->octis) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->octis) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->octis) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->octis) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->octis) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->octis) (2022.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->octis) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->octis) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->octis) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->octis) (3.0.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers->octis) (0.12.0+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers->octis) (4.64.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers->octis) (0.1.96)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers->octis) (4.19.2)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers->octis) (0.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->octis) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->octis) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->octis) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->octis) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->octis) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers->octis) (3.8.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->octis) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->octis) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->octis) (0.9.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->octis) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->octis) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->octis) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->octis) (1.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->octis) (3.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->octis) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->octis) (2.0.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers->octis) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import octis\n",
        "from octis.models.LDA import LDA\n",
        "from octis.models.NMF import NMF\n",
        "from octis.dataset.dataset import Dataset\n",
        "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
        "from octis.evaluation_metrics.coherence_metrics import Coherence"
      ],
      "metadata": {
        "id": "kkZuZyv8xUpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OCTIS -- LDA with crisis dataset"
      ],
      "metadata": {
        "id": "khItwfu-x7wb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our own data preprocessing method"
      ],
      "metadata": {
        "id": "leG3tOZk2hhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset()\n",
        "dataset.load_custom_dataset_from_folder(\"/content/drive/MyDrive/SS_2022_Praktikum/Crisis Dataset/Dataset_12\")"
      ],
      "metadata": {
        "id": "XmbXYf05yAAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LDA(num_topics=4)  # Create model\n",
        "output = model.train_model(dataset) # Train the model"
      ],
      "metadata": {
        "id": "3L9U-DqAyyOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See what are in the model output\n",
        "\n",
        "print(*list(output.keys()), sep=\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NRAqSSI1Xeo",
        "outputId": "0f844a24-fb87-4458-a9cf-8cf4b79edb91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topic-word-matrix\n",
            "topics\n",
            "topic-document-matrix\n",
            "test-topic-document-matrix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t in output['topics'][:]:\n",
        "  print(\" \".join(t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCZGh9tP1c8i",
        "outputId": "1de1dff8-babb-4a89-f8a2-c39cb9b34876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "earthquake felt wildfire feel smoke like time sf magnitude twitter\n",
            "flood wildfire mcmurray fort rain massive area california debbie heavy\n",
            "debbie cyclone flood earthquake queensland school ex flash go today\n",
            "california wildfire flood fire debbie northern cyclone least people home\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize metric\n",
        "npmi = Coherence(texts=dataset.get_corpus(), topk=10, measure='c_npmi')\n",
        "\n",
        "# Initialize metric\n",
        "topic_diversity = TopicDiversity(topk=10)"
      ],
      "metadata": {
        "id": "Ij_71YIi1rDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve metrics score\n",
        "topic_diversity_score = topic_diversity.score(output)\n",
        "print(\"Topic diversity: \" + str(topic_diversity_score))\n",
        "\n",
        "npmi_score = npmi.score(output)\n",
        "print(\"Coherence: \" + str(npmi_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPLZnHpi1uDT",
        "outputId": "0023ae27-460c-49c2-c113-87d29e8eb236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic diversity: 0.775\n",
            "Coherence: -0.091625380409654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With OCTIS' data preprocessing method (to do)"
      ],
      "metadata": {
        "id": "yQwHuvHE2mtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import os\n",
        "import string\n",
        "from octis.preprocessing.preprocessing import Preprocessing\n",
        "os.chdir(os.path.pardir)\n",
        "\n",
        "# Initialize preprocessing\n",
        "preprocessor = Preprocessing(vocabulary=None, max_features=None,\n",
        "                             remove_punctuation=True, punctuation=string.punctuation,\n",
        "                             lemmatize=True, stopword_list='english',\n",
        "                             min_chars=1, min_words_docs=0)\n",
        "# preprocess\n",
        "dataset = preprocessor.preprocess_dataset(documents_path=r'..\\corpus.txt', labels_path=r'..\\labels.txt')\n",
        "\n",
        "# save the preprocessed dataset\n",
        "dataset.save('hello_dataset')\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "8u6uQu6s2plf",
        "outputId": "474a4674-c661-4a24-f42b-54625048f87b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport os\\nimport string\\nfrom octis.preprocessing.preprocessing import Preprocessing\\nos.chdir(os.path.pardir)\\n\\n# Initialize preprocessing\\npreprocessor = Preprocessing(vocabulary=None, max_features=None,\\n                             remove_punctuation=True, punctuation=string.punctuation,\\n                             lemmatize=True, stopword_list='english',\\n                             min_chars=1, min_words_docs=0)\\n# preprocess\\ndataset = preprocessor.preprocess_dataset(documents_path=r'..\\\\corpus.txt', labels_path=r'..\\\\labels.txt')\\n\\n# save the preprocessed dataset\\ndataset.save('hello_dataset')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OCTIS -- LDA with 20News dataset"
      ],
      "metadata": {
        "id": "YmRQU4bE-7Xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset\n",
        "dataset = Dataset()\n",
        "dataset.fetch_dataset(\"20NewsGroup\")"
      ],
      "metadata": {
        "id": "srl5KGO94acd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Model\n",
        "model = LDA(num_topics=20, alpha=0.1)\n",
        "\n",
        "# Train the model\n",
        "output = model.train_model(dataset) "
      ],
      "metadata": {
        "id": "vtLxy4ry4kKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in output['topics'][:5]:\n",
        "  print(\" \".join(t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4NEC_sT46sX",
        "outputId": "054b98c1-fd89-4630-ff6c-49c31cd99c4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work window system problem drive run disk modem cable set\n",
            "list widget user include server send mail information application window\n",
            "book post read write font find mail article question text\n",
            "car launch space power sell make good chip satellite sale\n",
            "year time day doctor water drug start disease good health\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize metric\n",
        "npmi = Coherence(texts=dataset.get_corpus(), topk=10, measure='c_npmi')\n",
        "\n",
        "# Initialize metric\n",
        "topic_diversity = TopicDiversity(topk=10)"
      ],
      "metadata": {
        "id": "k7mtk_4t49qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve metrics score\n",
        "topic_diversity_score = topic_diversity.score(output)\n",
        "print(\"Topic diversity: \" + str(topic_diversity_score))\n",
        "\n",
        "npmi_score = npmi.score(output)\n",
        "print(\"Coherence: \" + str(npmi_score))\n",
        "\n",
        "# Strange, I got a different result as the example notebook"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YjzJL4_4_pc",
        "outputId": "1402d5ea-0bcc-42e0-89f0-c295c7ae7b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic diversity: 0.71\n",
            "Coherence: 0.06612433724770478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OCTIS -- NMF with crisis dataset"
      ],
      "metadata": {
        "id": "6u4xhVWO6Vrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset()\n",
        "dataset.load_custom_dataset_from_folder(\"/content/drive/MyDrive/SS_2022_Praktikum/Crisis Dataset/Dataset_12\")"
      ],
      "metadata": {
        "id": "TwR-zuJj6dFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NMF(num_topics=4)  # Create model\n",
        "output = model.train_model(dataset) # Train the model"
      ],
      "metadata": {
        "id": "p3w-U-uuPzR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in output['topics'][:]:\n",
        "  print(\" \".join(t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Paw9wp8qP0R-",
        "outputId": "ae77b579-9881-4f4a-8c9e-180525585e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flood flash rain warning heavy area water watch day people\n",
            "wildfire california fort mcmurray northern canada canadian alberta smoke via\n",
            "debbie cyclone ex queensland school tropical australia closed south flooding\n",
            "earthquake felt flood feel california fire today one like km\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize metric\n",
        "npmi = Coherence(texts=dataset.get_corpus(), topk=10, measure='c_npmi')\n",
        "\n",
        "# Initialize metric\n",
        "topic_diversity = TopicDiversity(topk=10)"
      ],
      "metadata": {
        "id": "Ldngy-hpP6y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve metrics score\n",
        "topic_diversity_score = topic_diversity.score(output)\n",
        "print(\"Topic diversity: \" + str(topic_diversity_score))\n",
        "\n",
        "npmi_score = npmi.score(output)\n",
        "print(\"Coherence: \" + str(npmi_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J8z5nktP8qz",
        "outputId": "e5b41ffe-26b3-4665-f79c-adb292566d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic diversity: 0.95\n",
            "Coherence: 0.07544957013802908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OCTIS -- NMF with 20News dataset"
      ],
      "metadata": {
        "id": "QKjlr5x3Qe-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset\n",
        "dataset = Dataset()\n",
        "dataset.fetch_dataset(\"20NewsGroup\")"
      ],
      "metadata": {
        "id": "Tuviks0qQk7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Model\n",
        "model = NMF(num_topics=20)\n",
        "\n",
        "# Train the model\n",
        "output = model.train_model(dataset) "
      ],
      "metadata": {
        "id": "GlI2MtL6QpOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in output['topics'][:5]:\n",
        "  print(\" \".join(t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5b274eeQw5h",
        "outputId": "ae4c2753-1ce2-41b4-8d3f-a610a0a9ce59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file system version software program display application set user read\n",
            "drive disk problem system hard administration question program government official\n",
            "key privacy internet encryption make information message post computer user\n",
            "window widget application server subject resource motif set include run\n",
            "graphic image package include send mail server object support datum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize metric\n",
        "npmi = Coherence(texts=dataset.get_corpus(), topk=10, measure='c_npmi')\n",
        "\n",
        "# Initialize metric\n",
        "topic_diversity = TopicDiversity(topk=10)"
      ],
      "metadata": {
        "id": "l77fOHFXRPb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve metrics score\n",
        "topic_diversity_score = topic_diversity.score(output)\n",
        "print(\"Topic diversity: \" + str(topic_diversity_score))\n",
        "\n",
        "npmi_score = npmi.score(output)\n",
        "print(\"Coherence: \" + str(npmi_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lELuVWZFRQ6A",
        "outputId": "dc1eef30-34f7-4a1a-c504-8cdcd1cf4596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic diversity: 0.705\n",
            "Coherence: 0.08307011633933507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NMF with sklearn (Very possibly give up)"
      ],
      "metadata": {
        "id": "fQPuG9EP-RT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "btZKPknpRU5t",
        "outputId": "c648a32a-ac35-4472-b74a-8aa637d67100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-a871fdc9ebee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups"
      ],
      "metadata": {
        "id": "NgcPgQzW-WUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download the datasets\n",
        "groups = fetch_20newsgroups()"
      ],
      "metadata": {
        "id": "HXCj4Ijj_fCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#type:sklearn.utils.Bunch\n",
        "groups.keys()"
      ],
      "metadata": {
        "id": "G_y2lZQp_jkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20 topics\n",
        "groups['target_names']"
      ],
      "metadata": {
        "id": "yQXwzFs4_6sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To which group the news belongs\n",
        "groups.target"
      ],
      "metadata": {
        "id": "1eeaKhdnADri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#It can be seen that the target ranges from 0 to 19\n",
        "import numpy as np\n",
        "np.unique(groups.target)"
      ],
      "metadata": {
        "id": "Ud_7rFdqAS6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The first news\n",
        "groups.data[0]"
      ],
      "metadata": {
        "id": "UzQv-BUkAS_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The first news belongs to the 8th topic\n",
        "groups.target[0]"
      ],
      "metadata": {
        "id": "z4R1iZ8wAtPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The 8th topic is ...\n",
        "groups.target_names[groups.target[0]]"
      ],
      "metadata": {
        "id": "EqJUFI0TA2WT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#How long is the first news\n",
        "len(groups.data[0])"
      ],
      "metadata": {
        "id": "DPufGf6KA-_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#And the second\n",
        "len(groups.data[1])"
      ],
      "metadata": {
        "id": "_0OeBTZsBDa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization"
      ],
      "metadata": {
        "id": "l55cPYCNBJ_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(groups.target)"
      ],
      "metadata": {
        "id": "8ycWC8NhBHSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "JBmnWhscBUqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#delete stop words and limit the num of features to 500, CountVectorizer is used to extract features\n",
        "cv = CountVectorizer(stop_words='english',max_features=500)\n",
        "transformed = cv.fit_transform(groups.data)\n",
        "#see what features are in the dataset\n",
        "print(cv.get_feature_names())"
      ],
      "metadata": {
        "id": "drvgcL-ABbgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape = Documents * Features\n",
        "print(np.shape(transformed))\n",
        "print('\\n')\n",
        "\n",
        "#see if the feature appears in sentence\n",
        "transformed.toarray()"
      ],
      "metadata": {
        "id": "5ZXac6-SCnWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#How many times does a feature appear\n",
        "sns.distplot(np.log(transformed.toarray().sum(axis=0)))\n",
        "plt.xlabel('Log Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution plot of 500 word counts')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Eb-u7YICDqjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "1lTASCosEzt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import names\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "IZAAiZ8_Ex1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#return True if astr only contains letters, otherwise, False\n",
        "def letters_only(astr) :\n",
        "  return astr.isalpha()"
      ],
      "metadata": {
        "id": "D7uncu42E3EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "#name dict\n",
        "nltk.download('names')\n",
        "cleaned=[]\n",
        "all_names = set(names.words())\n",
        "#Do lemmatization\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "aSDrTlx4E6Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "for post in groups.data:\n",
        "    for word in post.split():\n",
        "        if letters_only(word) and word not in all_names:\n",
        "            cleaned.append(' '.join([lemmatizer.lemmatize(word.lower())]))"
      ],
      "metadata": {
        "id": "5aRXv_oyFazQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#all the words in the dataset\n",
        "len(cleaned)"
      ],
      "metadata": {
        "id": "Zr5LZdfoN14l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed = cv.fit_transform(cleaned)\n",
        "print(cv.get_feature_names())"
      ],
      "metadata": {
        "id": "nXQVfCp8OQCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "clustering"
      ],
      "metadata": {
        "id": "ttK4v0BeOjwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import names\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "cNamtvJpOl2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_clusters = 20\n",
        "kmmodel = KMeans(n_clusters=number_of_clusters)\n",
        "kmmodel.fit(transformed)"
      ],
      "metadata": {
        "id": "RK1g1GX_Os66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "order_centroids = kmmodel.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = cv.get_feature_names()"
      ],
      "metadata": {
        "id": "ak7QZMbJPXdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(number_of_clusters):\n",
        "    print(\"Cluster %d:\" % i),\n",
        "    for ind in order_centroids[i, :10]:\n",
        "        print(' %s' % terms[ind])\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "9O7rcZgFPUGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NMF"
      ],
      "metadata": {
        "id": "4BQJsma0RKOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import names\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import NMF"
      ],
      "metadata": {
        "id": "1M_Tpk81RMq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d=5  # num topics\n",
        "clf = NMF(n_components=d, random_state=1).fit(transformed)"
      ],
      "metadata": {
        "id": "IkdVPL_5RMvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for topic_idex, topic in enumerate(clf.components_):\n",
        "  label = '{}: '.format(topic_idex)\n",
        "  print(label, \" \".join([cv.get_feature_names()[i]\n",
        "                         for i in topic.argsort()[::]]))"
      ],
      "metadata": {
        "id": "wY8QXnK1Rh0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install octis"
      ],
      "metadata": {
        "id": "JYytToSn_mhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import octis\n",
        "from octis.models.LDA import LDA\n",
        "from octis.dataset.dataset import Dataset\n",
        "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
        "from octis.evaluation_metrics.coherence_metrics import Coherence"
      ],
      "metadata": {
        "id": "Vt1KO7ab-_RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset\n",
        "dataset = Dataset()\n",
        "dataset.fetch_dataset(\"20NewsGroup\")"
      ],
      "metadata": {
        "id": "8bXp9TymAOMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.get_labels"
      ],
      "metadata": {
        "id": "wFr13S8UNj8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Model\n",
        "# Alpha is the parameter in Dirichlet distribution\n",
        "model = LDA(num_topics=20, alpha=0.1)"
      ],
      "metadata": {
        "id": "PE0WoFwWAPlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using default partitioning choice \n",
        "output = model.train_model(dataset)\n",
        "\n",
        "print(*list(output.keys()), sep=\"\\n\") # Print the output identifiers"
      ],
      "metadata": {
        "id": "C5pUV4xHDKWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#20 topics, each with 10 words\n",
        "np.shape(output['topics'])"
      ],
      "metadata": {
        "id": "HIQ65qFCEjb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(output['topic-document-matrix'])"
      ],
      "metadata": {
        "id": "vMp5U8ypE3LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in output['topics'][:5]:\n",
        "  print(\" \".join(t))"
      ],
      "metadata": {
        "id": "caOFbJPDDUcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize metric\n",
        "npmi = Coherence(texts=dataset.get_corpus(), topk=10, measure='c_npmi')"
      ],
      "metadata": {
        "id": "7MNAFCckDfB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize metric\n",
        "topic_diversity = TopicDiversity(topk=10)"
      ],
      "metadata": {
        "id": "gKGilQrfDg1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve metrics score\n",
        "topic_diversity_score = topic_diversity.score(output)\n",
        "print(\"Topic diversity: \"+str(topic_diversity_score))\n",
        "\n",
        "npmi_score = npmi.score(output)\n",
        "print(\"Coherence: \"+str(npmi_score))"
      ],
      "metadata": {
        "id": "282pfKL-Djt6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}