{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61510b1e",
   "metadata": {},
   "source": [
    "## Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80cf0947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Current Directory: \"/home/ferdi/topic-modeling-advancements\".\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.getcwd().endswith('/notebooks'):\n",
    "    os.chdir('..')\n",
    "\n",
    "assert os.path.exists('./src'), f\"[ERROR] The path src not detected in the current directory '{os.getcwd()}'.\"\n",
    "\n",
    "print(f'[INFO] Current Directory: \"{os.getcwd()}\".')\n",
    "\n",
    "if not os.path.exists(\"images\"):\n",
    "    os.mkdir(\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa868be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 11:21:11.864820: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-20 11:21:11.864957: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.models.LDA import LDA\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zlib\n",
    "import pickle as pkl\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import kaleido\n",
    "\n",
    "## Preencode data with embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "## Load data with datalaoder from src\n",
    "from src.utils import load_documents\n",
    "from src.bertopic_runner import Trainer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34bbd10",
   "metadata": {},
   "source": [
    "## Parametric run: BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88cc7239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ferdi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ferdi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ferdi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f68c08",
   "metadata": {},
   "source": [
    "If we set hdbscan_args to None, Kmeans is used for clustering. That way we can enforce having no noise clusters/documents.\n",
    "As a consequence we also have no assignment scores since KMeans does not produce any (set as default to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e10798",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'dataset': 'crisis_toy',\n",
    "    'preprocessing_funcs': [\n",
    "        'to_lowercase',\n",
    "        'standardize_accented_chars',\n",
    "        'remove_url',\n",
    "        'expand_contractions',\n",
    "        'remove_mentions',\n",
    "        'remove_hashtags',\n",
    "        'remove_new_lines',\n",
    "        'keep_only_alphabet',\n",
    "        # 'remove_extra_spaces',\n",
    "        'remove_english_stop_words',\n",
    "        'lemmatize_noun'\n",
    "    ],\n",
    "    'algorithm': 'bertopic',\n",
    "    'algorithm_args': {\n",
    "         \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "         \"top_n_words\": 10,\n",
    "         \"n_gram_range_tuple\": (1, 1),\n",
    "         ## Both the same as below\n",
    "         \"min_docs_per_topic\": 15,\n",
    "         \"number_topics\": 4,\n",
    "         ## Assign almost all docs to a topic\n",
    "         #\"no_noise\": True,\n",
    "         #\"prob_threshold\": 0.01,\n",
    "         ## Setting min_samples to reduce #docs classified as noise/in topic -1\n",
    "         \"hdbscan_args\": {\n",
    "                    \"min_cluster_size\": 15,\n",
    "                    \"metric\":'euclidean',\n",
    "                    \"cluster_selection_method\": 'eom',\n",
    "                    \"prediction_data\": True,\n",
    "                    #\"min_samples\": 15\n",
    "         },    \n",
    "        \"umap_args\": {\n",
    "                    \"n_neighbors\": 15,\n",
    "                    \"n_components\": 5,\n",
    "                    \"min_dist\": 0.0,\n",
    "                    \"metric\": 'cosine',\n",
    "                    \"low_memory\": False,\n",
    "                    \"random_state\": 42\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef95d8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Available Preprocessing Functions in the Module:['to_lowercase', 'standardize_accented_chars', 'remove_url', 'expand_missing_delimiter', 'remove_mentions', 'remove_hashtags', 'keep_only_alphabet', 'remove_new_lines', 'remove_extra_spaces', 'remove_html_tags', 'expand_contractions', 'remove_english_stop_words', 'lemmatize', 'lemmatize_verb', 'lemmatize_noun', 'lemmatize_adjective', 'correct_typo']\n",
      "[INFO] Preprocessing starting..\n",
      "[INFO] These string preprocessing methods will be applied to the data in order:\n",
      "(  'to_lowercase',\n",
      "   'standardize_accented_chars',\n",
      "   'remove_url',\n",
      "   'remove_mentions',\n",
      "   'remove_hashtags',\n",
      "   'remove_new_lines',\n",
      "   'keep_only_alphabet')\n",
      "[INFO] Then, these tokenized preprocessing methods will be applied to the data in order:\n",
      "(  '__tokenize',\n",
      "   'expand_contractions',\n",
      "   'remove_english_stop_words',\n",
      "   'lemmatize_noun',\n",
      "   '__glue')\n",
      "[INFO] Preprocessing completed in 5.488 seconds..\n",
      "Running with 4 topics\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07875520e9342a1990f7fc1873e8e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 11:22:06,279 - BERTopic - Reduced dimensionality\n",
      "2022-07-20 11:22:06,297 - BERTopic - Clustered reduced embeddings\n"
     ]
    }
   ],
   "source": [
    "##Maybe parallelism disable?\n",
    "#import os\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from src import preprocessor\n",
    "from src.utils import load_documents\n",
    "\n",
    "docs, labels = load_documents(dataset=configs['dataset'])\n",
    "\n",
    "if 'preprocessing_funcs' in configs:\n",
    "    docs = preprocessor.run(data=docs, prep_functions=configs['preprocessing_funcs'])\n",
    "    \n",
    "algorithm_args = configs['algorithm_args']\n",
    "algorithm_args.update(data_name=configs['dataset'],docs=docs,labels=labels)\n",
    "print(f'Running with {algorithm_args[\"number_topics\"]} topics')\n",
    "\n",
    "if configs['algorithm'] == 'bertopic':\n",
    "    # Encode data with embedding model\n",
    "    model = SentenceTransformer(algorithm_args['embedding_model'])\n",
    "    embeddings = model.encode(docs, show_progress_bar=True)\n",
    "    \n",
    "    trainer = Trainer(dataset = configs['dataset'],\n",
    "                      model_name = configs['algorithm'],\n",
    "                      params = algorithm_args,\n",
    "                      topk = algorithm_args[\"top_n_words\"],\n",
    "                      bt_embeddings = embeddings,\n",
    "                      )\n",
    "    \n",
    "    model, df_output_doc_topic, df_output_topic_word = trainer.train()\n",
    "    \n",
    "    from src.evaluator import compute_topic_scores\n",
    "    df_output_topic_word = compute_topic_scores(df_output_doc_topic, df_output_topic_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97fec096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>method</th>\n",
       "      <th>method_specific_params</th>\n",
       "      <th>dataset</th>\n",
       "      <th>num_given_topics</th>\n",
       "      <th>reduced</th>\n",
       "      <th>topic_num</th>\n",
       "      <th>topic_size</th>\n",
       "      <th>topic_words</th>\n",
       "      <th>word_scores</th>\n",
       "      <th>num_detected_topics</th>\n",
       "      <th>num_final_topics</th>\n",
       "      <th>duration_secs</th>\n",
       "      <th>diversity_unique</th>\n",
       "      <th>diversity_inv_rbo</th>\n",
       "      <th>coherence_npmi</th>\n",
       "      <th>coherence_v</th>\n",
       "      <th>rand_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1658308911</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'top_n...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>201</td>\n",
       "      <td>[flood, debbie, cyclone, rain, today, school, ...</td>\n",
       "      <td>[0.12487296520516175, 0.11498390801027375, 0.1...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>15.202784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.05357</td>\n",
       "      <td>0.409122</td>\n",
       "      <td>0.853663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1658308911</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'top_n...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>[earthquake, felt, feel, small, twitter, sf, t...</td>\n",
       "      <td>[0.5103614735047877, 0.1515561989210337, 0.069...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>15.202784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.05357</td>\n",
       "      <td>0.409122</td>\n",
       "      <td>0.853663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1658308911</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'top_n...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>[smoke, wildfire, fire, smell, california, lik...</td>\n",
       "      <td>[0.21855425061383502, 0.18618706849685615, 0.0...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>15.202784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.05357</td>\n",
       "      <td>0.409122</td>\n",
       "      <td>0.853663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       run_id    method                             method_specific_params  \\\n",
       "0  1658308911  bertopic  {'embedding_model': 'all-MiniLM-L6-v2', 'top_n...   \n",
       "1  1658308911  bertopic  {'embedding_model': 'all-MiniLM-L6-v2', 'top_n...   \n",
       "2  1658308911  bertopic  {'embedding_model': 'all-MiniLM-L6-v2', 'top_n...   \n",
       "\n",
       "      dataset  num_given_topics  reduced  topic_num  topic_size  \\\n",
       "0  crisis_toy                 4    False          0         201   \n",
       "1  crisis_toy                 4    False          1         101   \n",
       "2  crisis_toy                 4    False          2          94   \n",
       "\n",
       "                                         topic_words  \\\n",
       "0  [flood, debbie, cyclone, rain, today, school, ...   \n",
       "1  [earthquake, felt, feel, small, twitter, sf, t...   \n",
       "2  [smoke, wildfire, fire, smell, california, lik...   \n",
       "\n",
       "                                         word_scores  num_detected_topics  \\\n",
       "0  [0.12487296520516175, 0.11498390801027375, 0.1...                    3   \n",
       "1  [0.5103614735047877, 0.1515561989210337, 0.069...                    3   \n",
       "2  [0.21855425061383502, 0.18618706849685615, 0.0...                    3   \n",
       "\n",
       "   num_final_topics  duration_secs  diversity_unique  diversity_inv_rbo  \\\n",
       "0                 3      15.202784               1.0                1.0   \n",
       "1                 3      15.202784               1.0                1.0   \n",
       "2                 3      15.202784               1.0                1.0   \n",
       "\n",
       "   coherence_npmi  coherence_v  rand_index  \n",
       "0        -0.05357     0.409122    0.853663  \n",
       "1        -0.05357     0.409122    0.853663  \n",
       "2        -0.05357     0.409122    0.853663  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output_topic_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c40a7",
   "metadata": {},
   "source": [
    "## Parametric Run: LDA-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb47b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'dataset': 'crisis_toy',\n",
    "    'preprocessing_funcs': [\n",
    "        'to_lowercase',\n",
    "        'standardize_accented_chars',\n",
    "        'remove_url',\n",
    "        'expand_contractions',\n",
    "        'expand_missing_delimiter',\n",
    "        'remove_mentions',\n",
    "        'remove_hashtags',\n",
    "        'remove_new_lines',\n",
    "        'keep_only_alphabet',\n",
    "        # 'remove_extra_spaces',\n",
    "        'remove_english_stop_words',\n",
    "        'lemmatize_noun',\n",
    "        'correct_typo'\n",
    "    ],\n",
    "    'algorithm': 'lda-bert',\n",
    "    'algorithm_args': {\n",
    "            'embedding_model': \"all-MiniLM-L6-v2\",\n",
    "            'number_topics': 3,\n",
    "            'top_n_words': 10,\n",
    "            'gamma': 15 ,\n",
    "            'random_state': 42\n",
    "    }     \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b623da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import preprocessor\n",
    "from src.utils import load_documents\n",
    "\n",
    "docs, labels = load_documents(dataset=configs['dataset'])\n",
    "\n",
    "if 'preprocessing_funcs' in configs:\n",
    "    docs = preprocessor.run(data=docs, prep_functions=configs['preprocessing_funcs'])\n",
    "    \n",
    "algorithm_args = configs['algorithm_args']\n",
    "algorithm_args.update(data_name=configs['dataset'],docs=docs,labels=labels)\n",
    "print(f'Running with {algorithm_args[\"number_topics\"]} topics')\n",
    "\n",
    "if configs['algorithm'] == 'lda-bert':\n",
    "    # Encode data with embedding model\n",
    "    model = SentenceTransformer(algorithm_args['embedding_model'])\n",
    "    embeddings = model.encode(docs, show_progress_bar=True)\n",
    "    \n",
    "    trainer = Trainer(dataset = configs['dataset'],\n",
    "                      model_name = configs['algorithm'],\n",
    "                      params = algorithm_args,\n",
    "                      topk = algorithm_args[\"top_n_words\"],\n",
    "                      bt_embeddings = embeddings,\n",
    "                      )\n",
    "    \n",
    "    model, df_output_doc_topic, df_output_topic_word = trainer.train()\n",
    "    \n",
    "    from src.evaluator import compute_topic_scores\n",
    "    df_output_topic_word = compute_topic_scores(df_output_doc_topic, df_output_topic_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dba49ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>method</th>\n",
       "      <th>method_specific_params</th>\n",
       "      <th>dataset</th>\n",
       "      <th>num_given_topics</th>\n",
       "      <th>reduced</th>\n",
       "      <th>topic_num</th>\n",
       "      <th>topic_size</th>\n",
       "      <th>topic_words</th>\n",
       "      <th>word_scores</th>\n",
       "      <th>num_detected_topics</th>\n",
       "      <th>num_final_topics</th>\n",
       "      <th>duration_secs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>lda-bert</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'numbe...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>[debbie, cyclone, smoke, wildfire, earthquake,...</td>\n",
       "      <td>[0.041499330655957165, 0.038821954484605084, 0...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9.363553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>lda-bert</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'numbe...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>[flood, cyclone, debbie, rain, smoke, work, fl...</td>\n",
       "      <td>[0.06338028169014084, 0.01643192488262911, 0.0...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9.363553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>lda-bert</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'numbe...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>113</td>\n",
       "      <td>[earthquake, flood, fire, felt, smoke, califor...</td>\n",
       "      <td>[0.06869220607661823, 0.034346103038309116, 0....</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9.363553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       run_id    method                             method_specific_params  \\\n",
       "0  1658231276  lda-bert  {'embedding_model': 'all-MiniLM-L6-v2', 'numbe...   \n",
       "1  1658231276  lda-bert  {'embedding_model': 'all-MiniLM-L6-v2', 'numbe...   \n",
       "2  1658231276  lda-bert  {'embedding_model': 'all-MiniLM-L6-v2', 'numbe...   \n",
       "\n",
       "      dataset  num_given_topics  reduced  topic_num  topic_size  \\\n",
       "0  crisis_toy                 3    False          0         181   \n",
       "1  crisis_toy                 3    False          1         102   \n",
       "2  crisis_toy                 3    False          2         113   \n",
       "\n",
       "                                         topic_words  \\\n",
       "0  [debbie, cyclone, smoke, wildfire, earthquake,...   \n",
       "1  [flood, cyclone, debbie, rain, smoke, work, fl...   \n",
       "2  [earthquake, flood, fire, felt, smoke, califor...   \n",
       "\n",
       "                                         word_scores  num_detected_topics  \\\n",
       "0  [0.041499330655957165, 0.038821954484605084, 0...                    3   \n",
       "1  [0.06338028169014084, 0.01643192488262911, 0.0...                    3   \n",
       "2  [0.06869220607661823, 0.034346103038309116, 0....                    3   \n",
       "\n",
       "   num_final_topics  duration_secs  \n",
       "0                 3       9.363553  \n",
       "1                 3       9.363553  \n",
       "2                 3       9.363553  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output_topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fb2c569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>Document ID</th>\n",
       "      <th>Document</th>\n",
       "      <th>Real Label</th>\n",
       "      <th>Assigned Topic Num</th>\n",
       "      <th>Assignment Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>0</td>\n",
       "      <td>thereformedcrow nah going go earthquake</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>1</td>\n",
       "      <td>think earthquake</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>2</td>\n",
       "      <td>uhh else felt earthquake though</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>3</td>\n",
       "      <td>bay area nice size earthquake</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>4</td>\n",
       "      <td>thought dad farting turn earthquake</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>391</td>\n",
       "      <td>people keep asking good safe live cyclone debb...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>392</td>\n",
       "      <td>ayyeeee work got cancelled flood thank cyclone...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>393</td>\n",
       "      <td>jetstarairways helpful need change flight due ...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>394</td>\n",
       "      <td>getting hit ex tropical cyclone named debbie r...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>395</td>\n",
       "      <td>wet wet wet today great day stay indoors ex tr...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>396 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         run_id  Document ID  \\\n",
       "0    1658231276            0   \n",
       "1    1658231276            1   \n",
       "2    1658231276            2   \n",
       "3    1658231276            3   \n",
       "4    1658231276            4   \n",
       "..          ...          ...   \n",
       "391  1658231276          391   \n",
       "392  1658231276          392   \n",
       "393  1658231276          393   \n",
       "394  1658231276          394   \n",
       "395  1658231276          395   \n",
       "\n",
       "                                              Document  Real Label  \\\n",
       "0              thereformedcrow nah going go earthquake  earthquake   \n",
       "1                                     think earthquake  earthquake   \n",
       "2                      uhh else felt earthquake though  earthquake   \n",
       "3                        bay area nice size earthquake  earthquake   \n",
       "4                  thought dad farting turn earthquake  earthquake   \n",
       "..                                                 ...         ...   \n",
       "391  people keep asking good safe live cyclone debb...   hurricane   \n",
       "392  ayyeeee work got cancelled flood thank cyclone...   hurricane   \n",
       "393  jetstarairways helpful need change flight due ...   hurricane   \n",
       "394  getting hit ex tropical cyclone named debbie r...   hurricane   \n",
       "395  wet wet wet today great day stay indoors ex tr...   hurricane   \n",
       "\n",
       "     Assigned Topic Num  Assignment Score  \n",
       "0                     2                 1  \n",
       "1                     2                 1  \n",
       "2                     2                 1  \n",
       "3                     0                 1  \n",
       "4                     2                 1  \n",
       "..                  ...               ...  \n",
       "391                   0                 1  \n",
       "392                   1                 1  \n",
       "393                   0                 1  \n",
       "394                   0                 1  \n",
       "395                   0                 1  \n",
       "\n",
       "[396 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output_doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6eee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
