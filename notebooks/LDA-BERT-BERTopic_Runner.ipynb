{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61510b1e",
   "metadata": {},
   "source": [
    "## Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80cf0947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Current Directory: \"/home/ferdi/topic-modeling-advancements\".\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.getcwd().endswith('/notebooks'):\n",
    "    os.chdir('..')\n",
    "\n",
    "assert os.path.exists('./src'), f\"[ERROR] The path src not detected in the current directory '{os.getcwd()}'.\"\n",
    "\n",
    "print(f'[INFO] Current Directory: \"{os.getcwd()}\".')\n",
    "\n",
    "if not os.path.exists(\"images\"):\n",
    "    os.mkdir(\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa868be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-19 12:27:31.155007: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-19 12:27:31.155065: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.models.LDA import LDA\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zlib\n",
    "import pickle as pkl\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import kaleido\n",
    "\n",
    "## Preencode data with embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "## Load data with datalaoder from src\n",
    "from src.utils import load_documents\n",
    "from src.bertopic_runner import Trainer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6b262cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for model lda-bert, dataset crisis_toy and embedding all-MiniLM-L6-v2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1faedafb3bb246dbb9ec4f973ba3715d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing raw texts ...\n",
      "Preprocessing raw texts. Done!\n",
      "Clustering embeddings ...\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "Getting vector representations for BERT ...\n",
      "Getting vector representations for BERT. Done!\n",
      "Fitting Autoencoder ...\n",
      "Fitting Autoencoder Done!\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "Clustering embeddings. Done!\n",
      "the words scores are: [[0.03368683718028696, 0.029320024953212728, 0.021002287377833228, 0.020794343938448742, 0.019754626741526306, 0.019754626741526306, 0.018922852983988356, 0.017883135787065917, 0.016635475150758992, 0.01538781451445207], [0.04025423728813559, 0.030508474576271188, 0.02923728813559322, 0.025423728813559324, 0.02076271186440678, 0.019491525423728815, 0.019491525423728815, 0.015254237288135594, 0.014830508474576272, 0.013135593220338982]]\n",
      "and have length: 2\n",
      "Running experiment for model bertopic, dataset crisis_toy and embedding all-MiniLM-L6-v2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d627dda8a6b9429eb23ccedfe2c9f65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-19 09:42:08,855 - BERTopic - Reduced dimensionality\n",
      "2022-07-19 09:42:08,877 - BERTopic - Clustered reduced embeddings\n",
      "2022-07-19 09:42:09,054 - BERTopic - Reduced number of topics from 3 to 2\n"
     ]
    }
   ],
   "source": [
    "## Embedding models to try\n",
    "#list_emb_models = [\"all-mpnet-base-v2\", \"all-distilroberta-v1\", \"all-MiniLM-L12-v2\", \"all-MiniLM-L6-v2\"]\n",
    "list_emb_models = [\"all-MiniLM-L6-v2\"]\n",
    "list_emb_models.reverse()\n",
    "\n",
    "## Datasets to try\n",
    "#list_data_sets = [\"crisis_12\", \"crisis_12_preprocessed\", \"crisis_1\", \"crisis_1_preprocessed\", \"20news\"]\n",
    "list_data_sets = [\"crisis_toy\"]\n",
    "\n",
    "## Models to try\n",
    "list_models = ['lda-bert', \"bertopic\"]\n",
    "#Nr topics\n",
    "nr_topics_dict = {\"crisis_1\": 12,\n",
    "                  \"crisis_1_preprocessed\": 12,\n",
    "                  \"crisis_12\": 10,\n",
    "                  \"crisis_12_preprocessed\": 4,\n",
    "                  \"20news\": 20,\n",
    "                  \"crisis_toy\": 2}\n",
    "\n",
    "experiment_results = []\n",
    "\n",
    "for m in list_models:\n",
    "    for e in list_emb_models:\n",
    "        for dataset in list_data_sets:\n",
    "            print(f'Running experiment for model {m}, dataset {dataset} and embedding {e}.')\n",
    "            \n",
    "            # Load data\n",
    "            data, labels = load_documents(dataset)\n",
    "            \n",
    "            # Encode data with embedding model\n",
    "            model = SentenceTransformer(e)\n",
    "            embeddings = model.encode(data, show_progress_bar=True)\n",
    "            \n",
    "            ## Set params for respective model\n",
    "            if m==\"lda-bert\":\n",
    "                params = {\n",
    "                    'embedding_model': e,\n",
    "                    'number_topics': nr_topics_dict[dataset],\n",
    "                    'top_n_words': 10,\n",
    "                    'gamma': 15 \n",
    "                }\n",
    "            elif m==\"bertopic\":\n",
    "                params = {\n",
    "                    \"embedding_model\": e,\n",
    "                    \"top_n_words\": 10,\n",
    "                    \"n_gram_range_tuple\": (1, 1),\n",
    "                    \"min_docs_per_topic\": 15,\n",
    "                    \"number_topics\": nr_topics_dict[dataset],\n",
    "                    ## Assign almost all docs to a topic\n",
    "                    \"no_noise\": True,\n",
    "                    \"prob_threshold\": 0.01\n",
    "                }\n",
    "            \n",
    "                ## Setting min_samples to reduce #docs classified as noise/in topic -1\n",
    "                hdbscan_args = {\n",
    "                    \"min_cluster_size\": params[\"min_docs_per_topic\"],\n",
    "                    \"metric\":'euclidean',\n",
    "                    \"cluster_selection_method\": 'eom',\n",
    "                    \"prediction_data\": True,\n",
    "                    \"min_samples\": 1\n",
    "                }\n",
    "                \n",
    "                umap_args = {\n",
    "                    \"n_neighbors\": 15,\n",
    "                    \"n_components\": 5,\n",
    "                    \"min_dist\": 0.0,\n",
    "                    \"metric\": 'cosine',\n",
    "                    \"low_memory\": False\n",
    "                }\n",
    "                \n",
    "                params['hdbscan_args'] = hdbscan_args\n",
    "                params['umap_args'] = umap_args\n",
    "            \n",
    "            trainer = Trainer(dataset = dataset,\n",
    "                              model_name = m,\n",
    "                              params = params,\n",
    "                              topk = 10,\n",
    "                              bt_embeddings = embeddings,\n",
    "                              custom_model = None,\n",
    "                              verbose = True,\n",
    "                              )\n",
    "            \n",
    "            model, df_output_doc_topic, df_output_topic_word = trainer.train()\n",
    "            \n",
    "            # Append row as tuple to a list to create dataframe later\n",
    "            experiment_results.append((model, df_output_doc_topic, df_output_topic_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ef4f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda-bert \n",
    "model, df_output_doc_topic, df_output_topic_word = experiment_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fdb0385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>method</th>\n",
       "      <th>method_specific_params</th>\n",
       "      <th>dataset</th>\n",
       "      <th>num_given_topics</th>\n",
       "      <th>reduced</th>\n",
       "      <th>topic_num</th>\n",
       "      <th>topic_size</th>\n",
       "      <th>topic_words</th>\n",
       "      <th>word_scores</th>\n",
       "      <th>num_detected_topics</th>\n",
       "      <th>num_final_topics</th>\n",
       "      <th>duration_secs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1658216488</td>\n",
       "      <td>lda-bert</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'numbe...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>278</td>\n",
       "      <td>[the, ., :, I, in, https, earthquake, to, ,, a]</td>\n",
       "      <td>[0.03368683718028696, 0.029320024953212728, 0....</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10.47802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1658216488</td>\n",
       "      <td>lda-bert</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'numbe...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "      <td>[#, ., :, https, !, ,, the, in, to, of]</td>\n",
       "      <td>[0.04025423728813559, 0.030508474576271188, 0....</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10.47802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       run_id    method                             method_specific_params  \\\n",
       "0  1658216488  lda-bert  {'embedding_model': 'all-MiniLM-L6-v2', 'numbe...   \n",
       "1  1658216488  lda-bert  {'embedding_model': 'all-MiniLM-L6-v2', 'numbe...   \n",
       "\n",
       "      dataset  num_given_topics  reduced  topic_num  topic_size  \\\n",
       "0  crisis_toy                 2    False          0         278   \n",
       "1  crisis_toy                 2    False          1         118   \n",
       "\n",
       "                                       topic_words  \\\n",
       "0  [the, ., :, I, in, https, earthquake, to, ,, a]   \n",
       "1          [#, ., :, https, !, ,, the, in, to, of]   \n",
       "\n",
       "                                         word_scores  num_detected_topics  \\\n",
       "0  [0.03368683718028696, 0.029320024953212728, 0....                    2   \n",
       "1  [0.04025423728813559, 0.030508474576271188, 0....                    2   \n",
       "\n",
       "   num_final_topics  duration_secs  \n",
       "0                 2       10.47802  \n",
       "1                 2       10.47802  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output_topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57854c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bertopic\n",
    "model, df_output_doc_topic, df_output_topic_word = experiment_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f95562c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>method</th>\n",
       "      <th>method_specific_params</th>\n",
       "      <th>dataset</th>\n",
       "      <th>num_given_topics</th>\n",
       "      <th>reduced</th>\n",
       "      <th>topic_num</th>\n",
       "      <th>topic_size</th>\n",
       "      <th>topic_words</th>\n",
       "      <th>word_scores</th>\n",
       "      <th>num_detected_topics</th>\n",
       "      <th>num_final_topics</th>\n",
       "      <th>duration_secs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1658216513</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'top_n...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>296</td>\n",
       "      <td>[the, co, https, to, in, is, flood, debbie, of...</td>\n",
       "      <td>[0.106000510582472, 0.08556013489946064, 0.085...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>15.908521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1658216513</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'top_n...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>[earthquake, was, an, that, just, felt, in, th...</td>\n",
       "      <td>[0.4025949207930253, 0.21330151994995877, 0.18...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>15.908521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       run_id    method                             method_specific_params  \\\n",
       "0  1658216513  bertopic  {'embedding_model': 'all-MiniLM-L6-v2', 'top_n...   \n",
       "1  1658216513  bertopic  {'embedding_model': 'all-MiniLM-L6-v2', 'top_n...   \n",
       "\n",
       "      dataset  num_given_topics  reduced  topic_num  topic_size  \\\n",
       "0  crisis_toy                 2     True          0         296   \n",
       "1  crisis_toy                 2     True          1         100   \n",
       "\n",
       "                                         topic_words  \\\n",
       "0  [the, co, https, to, in, is, flood, debbie, of...   \n",
       "1  [earthquake, was, an, that, just, felt, in, th...   \n",
       "\n",
       "                                         word_scores  num_detected_topics  \\\n",
       "0  [0.106000510582472, 0.08556013489946064, 0.085...                    3   \n",
       "1  [0.4025949207930253, 0.21330151994995877, 0.18...                    3   \n",
       "\n",
       "   num_final_topics  duration_secs  \n",
       "0                 2      15.908521  \n",
       "1                 2      15.908521  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output_topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2985577c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HDBSCAN(min_cluster_size=15, min_samples=1, prediction_data=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hdbscan_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34bbd10",
   "metadata": {},
   "source": [
    "## Parametric run: BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88cc7239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ferdi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ferdi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ferdi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f68c08",
   "metadata": {},
   "source": [
    "If we set hdbscan_args to None, Kmeans is used for clustering. That way we can enforce having no noise clusters/documents.\n",
    "As a consequence we also have no assignment scores since KMeans does not produce any (set as default to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31e10798",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'dataset': 'crisis_toy',\n",
    "    'preprocessing_funcs': [\n",
    "        'to_lowercase',\n",
    "        'standardize_accented_chars',\n",
    "        'remove_url',\n",
    "        'expand_contractions',\n",
    "        'remove_mentions',\n",
    "        'remove_hashtags',\n",
    "        'remove_new_lines',\n",
    "        'keep_only_alphabet',\n",
    "        # 'remove_extra_spaces',\n",
    "        'remove_english_stop_words',\n",
    "        'lemmatize_noun'\n",
    "    ],\n",
    "    'algorithm': 'bertopic',\n",
    "    'algorithm_args': {\n",
    "         \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "         \"top_n_words\": 10,\n",
    "         \"n_gram_range_tuple\": (1, 1),\n",
    "         ## Both the same as below\n",
    "         \"min_docs_per_topic\": 15,\n",
    "         \"number_topics\": 4,\n",
    "         ## Assign almost all docs to a topic\n",
    "         #\"no_noise\": True,\n",
    "         #\"prob_threshold\": 0.01,\n",
    "         ## Setting min_samples to reduce #docs classified as noise/in topic -1\n",
    "         \"hdbscan_args\": {\n",
    "                    \"min_cluster_size\": 15,\n",
    "                    \"metric\":'euclidean',\n",
    "                    \"cluster_selection_method\": 'eom',\n",
    "                    \"prediction_data\": True,\n",
    "                    #\"min_samples\": 15\n",
    "         },    \n",
    "        \"umap_args\": {\n",
    "                    \"n_neighbors\": 15,\n",
    "                    \"n_components\": 5,\n",
    "                    \"min_dist\": 0.0,\n",
    "                    \"metric\": 'cosine',\n",
    "                    \"low_memory\": False,\n",
    "                    \"random_state\": 42\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef95d8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Available Preprocessing Functions in the Module:['to_lowercase', 'standardize_accented_chars', 'remove_url', 'expand_contractions', 'remove_mentions', 'remove_hashtags', 'keep_only_alphabet', 'remove_new_lines', 'remove_extra_spaces', 'remove_html_tags', 'remove_english_stop_words', 'lemmatize', 'lemmatize_verb', 'lemmatize_noun', 'lemmatize_adjective']\n",
      "[INFO] Preprocessing starting..\n",
      "[INFO] These string preprocessing methods will be applied to the data in order:\n",
      "(  'to_lowercase',\n",
      "   'standardize_accented_chars',\n",
      "   'remove_url',\n",
      "   'expand_contractions',\n",
      "   'remove_mentions',\n",
      "   'remove_hashtags',\n",
      "   'remove_new_lines',\n",
      "   'keep_only_alphabet')\n",
      "[INFO] Then, these tokenized preprocessing methods will be applied to the data in order:\n",
      "(  '__tokenize',\n",
      "   'remove_english_stop_words',\n",
      "   'lemmatize_noun',\n",
      "   '__glue')\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[INFO] Preprocessing completed in 5.839 seconds..\n",
      "Running with 4 topics\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fff375c31f34fbcaea850c462aa1670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-19 12:30:06,358 - BERTopic - Reduced dimensionality\n",
      "2022-07-19 12:30:06,386 - BERTopic - Clustered reduced embeddings\n",
      "2022-07-19 12:30:06,999 - BERTopic - Reduced number of topics from 5 to 5\n"
     ]
    }
   ],
   "source": [
    "##Maybe parallelism disable?\n",
    "#import os\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from src import preprocessor\n",
    "from src.utils import load_documents\n",
    "\n",
    "docs, labels = load_documents(dataset=configs['dataset'])\n",
    "\n",
    "if 'preprocessing_funcs' in configs:\n",
    "    docs = preprocessor.run(data=docs, prep_functions=configs['preprocessing_funcs'])\n",
    "    \n",
    "algorithm_args = configs['algorithm_args']\n",
    "algorithm_args.update(data_name=configs['dataset'],docs=docs,labels=labels)\n",
    "print(f'Running with {algorithm_args[\"number_topics\"]} topics')\n",
    "\n",
    "if configs['algorithm'] == 'bertopic':\n",
    "    # Encode data with embedding model\n",
    "    model = SentenceTransformer(algorithm_args['embedding_model'])\n",
    "    embeddings = model.encode(docs, show_progress_bar=True)\n",
    "    \n",
    "    trainer = Trainer(dataset = configs['dataset'],\n",
    "                      model_name = configs['algorithm'],\n",
    "                      params = algorithm_args,\n",
    "                      topk = algorithm_args[\"top_n_words\"],\n",
    "                      bt_embeddings = embeddings,\n",
    "                      )\n",
    "    \n",
    "    model, df_output_doc_topic, df_output_topic_word = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97fec096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>method</th>\n",
       "      <th>method_specific_params</th>\n",
       "      <th>dataset</th>\n",
       "      <th>num_given_topics</th>\n",
       "      <th>reduced</th>\n",
       "      <th>topic_num</th>\n",
       "      <th>topic_size</th>\n",
       "      <th>topic_words</th>\n",
       "      <th>word_scores</th>\n",
       "      <th>num_detected_topics</th>\n",
       "      <th>num_final_topics</th>\n",
       "      <th>duration_secs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1658226595</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'top_n...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>[flood, rain, water, basement, flash, heavy, f...</td>\n",
       "      <td>[0.17884271362411105, 0.05607179255504992, 0.0...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>11.276808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1658226595</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'top_n...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>[earthquake, felt, feel, small, twitter, sf, t...</td>\n",
       "      <td>[0.4155759390043396, 0.13284892105804227, 0.06...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>11.276808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1658226595</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'top_n...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>95</td>\n",
       "      <td>[smoke, wildfire, fire, smell, california, lik...</td>\n",
       "      <td>[0.1808532212759992, 0.15586154467642044, 0.07...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>11.276808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1658226595</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'top_n...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>83</td>\n",
       "      <td>[debbie, cyclone, school, ex, closed, cancelle...</td>\n",
       "      <td>[0.2168539266597446, 0.21021521924112715, 0.06...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>11.276808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1658226595</td>\n",
       "      <td>bertopic</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'top_n...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>[windy, shitttttt, zacharylevi, holyy, wonder,...</td>\n",
       "      <td>[0.5348637446613509, 0.5348637446613509, 0.534...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>11.276808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       run_id    method                             method_specific_params  \\\n",
       "0  1658226595  bertopic  {'embedding_model': 'all-MiniLM-L6-v2', 'top_n...   \n",
       "1  1658226595  bertopic  {'embedding_model': 'all-MiniLM-L6-v2', 'top_n...   \n",
       "2  1658226595  bertopic  {'embedding_model': 'all-MiniLM-L6-v2', 'top_n...   \n",
       "3  1658226595  bertopic  {'embedding_model': 'all-MiniLM-L6-v2', 'top_n...   \n",
       "4  1658226595  bertopic  {'embedding_model': 'all-MiniLM-L6-v2', 'top_n...   \n",
       "\n",
       "      dataset  num_given_topics  reduced  topic_num  topic_size  \\\n",
       "0  crisis_toy                 4     True          0         114   \n",
       "1  crisis_toy                 4     True          1         102   \n",
       "2  crisis_toy                 4     True          2          95   \n",
       "3  crisis_toy                 4     True          3          83   \n",
       "4  crisis_toy                 4     True         -1           2   \n",
       "\n",
       "                                         topic_words  \\\n",
       "0  [flood, rain, water, basement, flash, heavy, f...   \n",
       "1  [earthquake, felt, feel, small, twitter, sf, t...   \n",
       "2  [smoke, wildfire, fire, smell, california, lik...   \n",
       "3  [debbie, cyclone, school, ex, closed, cancelle...   \n",
       "4  [windy, shitttttt, zacharylevi, holyy, wonder,...   \n",
       "\n",
       "                                         word_scores  num_detected_topics  \\\n",
       "0  [0.17884271362411105, 0.05607179255504992, 0.0...                    5   \n",
       "1  [0.4155759390043396, 0.13284892105804227, 0.06...                    5   \n",
       "2  [0.1808532212759992, 0.15586154467642044, 0.07...                    5   \n",
       "3  [0.2168539266597446, 0.21021521924112715, 0.06...                    5   \n",
       "4  [0.5348637446613509, 0.5348637446613509, 0.534...                    5   \n",
       "\n",
       "   num_final_topics  duration_secs  \n",
       "0                 5      11.276808  \n",
       "1                 5      11.276808  \n",
       "2                 5      11.276808  \n",
       "3                 5      11.276808  \n",
       "4                 5      11.276808  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output_topic_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c40a7",
   "metadata": {},
   "source": [
    "## Parametric Run: LDA-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03eb47b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'dataset': 'crisis_toy',\n",
    "    'preprocessing_funcs': [\n",
    "        'to_lowercase',\n",
    "        'standardize_accented_chars',\n",
    "        'remove_url',\n",
    "        'expand_contractions',\n",
    "        'expand_missing_delimiter',\n",
    "        'remove_mentions',\n",
    "        'remove_hashtags',\n",
    "        'remove_new_lines',\n",
    "        'keep_only_alphabet',\n",
    "        # 'remove_extra_spaces',\n",
    "        'remove_english_stop_words',\n",
    "        'lemmatize_noun',\n",
    "        'correct_typo'\n",
    "    ],\n",
    "    'algorithm': 'lda-bert',\n",
    "    'algorithm_args': {\n",
    "            'embedding_model': \"all-MiniLM-L6-v2\",\n",
    "            'number_topics': 3,\n",
    "            'top_n_words': 10,\n",
    "            'gamma': 15 \n",
    "    }     \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3b623da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Available Preprocessing Functions in the Module:['to_lowercase', 'standardize_accented_chars', 'remove_url', 'expand_contractions', 'expand_missing_delimiter', 'remove_mentions', 'remove_hashtags', 'keep_only_alphabet', 'remove_new_lines', 'remove_extra_spaces', 'remove_html_tags', 'remove_english_stop_words', 'lemmatize', 'lemmatize_verb', 'lemmatize_noun', 'lemmatize_adjective', 'correct_typo']\n",
      "[INFO] Preprocessing starting..\n",
      "[INFO] These string preprocessing methods will be applied to the data in order:\n",
      "(  'to_lowercase',\n",
      "   'standardize_accented_chars',\n",
      "   'remove_url',\n",
      "   'expand_contractions',\n",
      "   'expand_missing_delimiter',\n",
      "   'remove_mentions',\n",
      "   'remove_hashtags',\n",
      "   'remove_new_lines',\n",
      "   'keep_only_alphabet')\n",
      "[INFO] Then, these tokenized preprocessing methods will be applied to the data in order:\n",
      "(  '__tokenize',\n",
      "   'remove_english_stop_words',\n",
      "   'lemmatize_noun',\n",
      "   'correct_typo',\n",
      "   '__glue')\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[INFO] Preprocessing completed in 10.505 seconds..\n",
      "Running with 3 topics\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d9c92057024576a0444a4e15528833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing raw texts ...\n",
      "Preprocessing raw texts. Done!\n",
      "Clustering embeddings ...\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "Getting vector representations for BERT ...\n",
      "Getting vector representations for BERT. Done!\n",
      "Fitting Autoencoder ...\n",
      "Fitting Autoencoder Done!\n",
      "13/13 [==============================] - 0s 4ms/step\n",
      "Clustering embeddings. Done!\n",
      "the words scores are: [[0.09523809523809523, 0.023809523809523808, 0.022675736961451247, 0.015873015873015872, 0.015873015873015872, 0.013605442176870748, 0.013605442176870748, 0.01020408163265306, 0.009070294784580499, 0.007936507936507936], [0.03349673202614379, 0.03022875816993464, 0.028594771241830064, 0.0196078431372549, 0.0196078431372549, 0.01715686274509804, 0.014705882352941176, 0.00980392156862745, 0.008986928104575163, 0.008986928104575163], [0.055165496489468405, 0.05215646940822467, 0.03610832497492478, 0.01905717151454363, 0.013039117352056168, 0.011033099297893681, 0.010030090270812437, 0.010030090270812437, 0.009027081243731194, 0.009027081243731194]]\n",
      "and have length: 3\n"
     ]
    }
   ],
   "source": [
    "from src import preprocessor\n",
    "from src.utils import load_documents\n",
    "\n",
    "docs, labels = load_documents(dataset=configs['dataset'])\n",
    "\n",
    "if 'preprocessing_funcs' in configs:\n",
    "    docs = preprocessor.run(data=docs, prep_functions=configs['preprocessing_funcs'])\n",
    "    \n",
    "algorithm_args = configs['algorithm_args']\n",
    "algorithm_args.update(data_name=configs['dataset'],docs=docs,labels=labels)\n",
    "print(f'Running with {algorithm_args[\"number_topics\"]} topics')\n",
    "\n",
    "if configs['algorithm'] == 'lda-bert':\n",
    "    # Encode data with embedding model\n",
    "    model = SentenceTransformer(algorithm_args['embedding_model'])\n",
    "    embeddings = model.encode(docs, show_progress_bar=True)\n",
    "    \n",
    "    trainer = Trainer(dataset = configs['dataset'],\n",
    "                      model_name = configs['algorithm'],\n",
    "                      params = algorithm_args,\n",
    "                      topk = algorithm_args[\"top_n_words\"],\n",
    "                      bt_embeddings = embeddings,\n",
    "                      )\n",
    "    \n",
    "    model, df_output_doc_topic, df_output_topic_word = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dba49ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>method</th>\n",
       "      <th>method_specific_params</th>\n",
       "      <th>dataset</th>\n",
       "      <th>num_given_topics</th>\n",
       "      <th>reduced</th>\n",
       "      <th>topic_num</th>\n",
       "      <th>topic_size</th>\n",
       "      <th>topic_words</th>\n",
       "      <th>word_scores</th>\n",
       "      <th>num_detected_topics</th>\n",
       "      <th>num_final_topics</th>\n",
       "      <th>duration_secs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>lda-bert</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'numbe...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>[debbie, cyclone, smoke, wildfire, earthquake,...</td>\n",
       "      <td>[0.041499330655957165, 0.038821954484605084, 0...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9.363553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>lda-bert</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'numbe...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>[flood, cyclone, debbie, rain, smoke, work, fl...</td>\n",
       "      <td>[0.06338028169014084, 0.01643192488262911, 0.0...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9.363553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>lda-bert</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'numbe...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>113</td>\n",
       "      <td>[earthquake, flood, fire, felt, smoke, califor...</td>\n",
       "      <td>[0.06869220607661823, 0.034346103038309116, 0....</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9.363553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       run_id    method                             method_specific_params  \\\n",
       "0  1658231276  lda-bert  {'embedding_model': 'all-MiniLM-L6-v2', 'numbe...   \n",
       "1  1658231276  lda-bert  {'embedding_model': 'all-MiniLM-L6-v2', 'numbe...   \n",
       "2  1658231276  lda-bert  {'embedding_model': 'all-MiniLM-L6-v2', 'numbe...   \n",
       "\n",
       "      dataset  num_given_topics  reduced  topic_num  topic_size  \\\n",
       "0  crisis_toy                 3    False          0         181   \n",
       "1  crisis_toy                 3    False          1         102   \n",
       "2  crisis_toy                 3    False          2         113   \n",
       "\n",
       "                                         topic_words  \\\n",
       "0  [debbie, cyclone, smoke, wildfire, earthquake,...   \n",
       "1  [flood, cyclone, debbie, rain, smoke, work, fl...   \n",
       "2  [earthquake, flood, fire, felt, smoke, califor...   \n",
       "\n",
       "                                         word_scores  num_detected_topics  \\\n",
       "0  [0.041499330655957165, 0.038821954484605084, 0...                    3   \n",
       "1  [0.06338028169014084, 0.01643192488262911, 0.0...                    3   \n",
       "2  [0.06869220607661823, 0.034346103038309116, 0....                    3   \n",
       "\n",
       "   num_final_topics  duration_secs  \n",
       "0                 3       9.363553  \n",
       "1                 3       9.363553  \n",
       "2                 3       9.363553  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output_topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fb2c569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>Document ID</th>\n",
       "      <th>Document</th>\n",
       "      <th>Real Label</th>\n",
       "      <th>Assigned Topic Num</th>\n",
       "      <th>Assignment Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>0</td>\n",
       "      <td>thereformedcrow nah going go earthquake</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>1</td>\n",
       "      <td>think earthquake</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>2</td>\n",
       "      <td>uhh else felt earthquake though</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>3</td>\n",
       "      <td>bay area nice size earthquake</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>4</td>\n",
       "      <td>thought dad farting turn earthquake</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>391</td>\n",
       "      <td>people keep asking good safe live cyclone debb...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>392</td>\n",
       "      <td>ayyeeee work got cancelled flood thank cyclone...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>393</td>\n",
       "      <td>jetstarairways helpful need change flight due ...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>394</td>\n",
       "      <td>getting hit ex tropical cyclone named debbie r...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>395</td>\n",
       "      <td>wet wet wet today great day stay indoors ex tr...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>396 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         run_id  Document ID  \\\n",
       "0    1658231276            0   \n",
       "1    1658231276            1   \n",
       "2    1658231276            2   \n",
       "3    1658231276            3   \n",
       "4    1658231276            4   \n",
       "..          ...          ...   \n",
       "391  1658231276          391   \n",
       "392  1658231276          392   \n",
       "393  1658231276          393   \n",
       "394  1658231276          394   \n",
       "395  1658231276          395   \n",
       "\n",
       "                                              Document  Real Label  \\\n",
       "0              thereformedcrow nah going go earthquake  earthquake   \n",
       "1                                     think earthquake  earthquake   \n",
       "2                      uhh else felt earthquake though  earthquake   \n",
       "3                        bay area nice size earthquake  earthquake   \n",
       "4                  thought dad farting turn earthquake  earthquake   \n",
       "..                                                 ...         ...   \n",
       "391  people keep asking good safe live cyclone debb...   hurricane   \n",
       "392  ayyeeee work got cancelled flood thank cyclone...   hurricane   \n",
       "393  jetstarairways helpful need change flight due ...   hurricane   \n",
       "394  getting hit ex tropical cyclone named debbie r...   hurricane   \n",
       "395  wet wet wet today great day stay indoors ex tr...   hurricane   \n",
       "\n",
       "     Assigned Topic Num  Assignment Score  \n",
       "0                     2                 1  \n",
       "1                     2                 1  \n",
       "2                     2                 1  \n",
       "3                     0                 1  \n",
       "4                     2                 1  \n",
       "..                  ...               ...  \n",
       "391                   0                 1  \n",
       "392                   1                 1  \n",
       "393                   0                 1  \n",
       "394                   0                 1  \n",
       "395                   0                 1  \n",
       "\n",
       "[396 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output_doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6eee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
