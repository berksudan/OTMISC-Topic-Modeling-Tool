{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61510b1e",
   "metadata": {},
   "source": [
    "## Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80cf0947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Current Directory: \"/home/ferdi/topic-modeling-advancements\".\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.getcwd().endswith('/notebooks'):\n",
    "    os.chdir('..')\n",
    "\n",
    "assert os.path.exists('./src'), f\"[ERROR] The path src not detected in the current directory '{os.getcwd()}'.\"\n",
    "\n",
    "print(f'[INFO] Current Directory: \"{os.getcwd()}\".')\n",
    "\n",
    "if not os.path.exists(\"images\"):\n",
    "    os.mkdir(\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa868be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 11:21:11.864820: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-20 11:21:11.864957: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.models.LDA import LDA\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zlib\n",
    "import pickle as pkl\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import kaleido\n",
    "\n",
    "## Preencode data with embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "## Load data with datalaoder from src\n",
    "from src.utils import load_documents\n",
    "from src.bertopic_runner import Trainer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34bbd10",
   "metadata": {},
   "source": [
    "## Parametric run: BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88cc7239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ferdi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ferdi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ferdi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f68c08",
   "metadata": {},
   "source": [
    "If we set hdbscan_args to None, Kmeans is used for clustering. That way we can enforce having no noise clusters/documents.\n",
    "As a consequence we also have no assignment scores since KMeans does not produce any (set as default to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31e10798",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1948206967.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [13]\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"hdbscan_args\": {\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "configs = {\n",
    "    'dataset': 'crisis_toy',\n",
    "    'preprocessing_funcs': [\n",
    "        'to_lowercase',\n",
    "        'standardize_accented_chars',\n",
    "        'remove_url',\n",
    "        'expand_contractions',\n",
    "        'remove_mentions',\n",
    "        'remove_hashtags',\n",
    "        'remove_new_lines',\n",
    "        'keep_only_alphabet',\n",
    "        # 'remove_extra_spaces',\n",
    "        'remove_english_stop_words',\n",
    "        'lemmatize_noun'\n",
    "    ],\n",
    "    'algorithm': 'bertopic',\n",
    "    'algorithm_args': {\n",
    "        \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "        \"top_n_words\": 10,\n",
    "        \"n_gram_range_tuple\": (1, 1),\n",
    "        ## Both the same as below\n",
    "        \"min_docs_per_topic\": 15,\n",
    "        \"number_topics\": 4,\n",
    "         ## Assign almost all docs to a topic\n",
    "         #\"no_noise\": True,\n",
    "         #\"prob_threshold\": 0.01,\n",
    "         ## Setting min_samples to reduce #docs classified as noise/in topic -1\n",
    "        \"cluster_model\": \"kmeans\" #\"hdbscan\"or \"kmeans\" then we do not need hdbscan_args; if used are ignored\n",
    "        \"hdbscan_args\": {\n",
    "                    \"min_cluster_size\": 15,\n",
    "                    \"metric\":'euclidean',\n",
    "                    \"cluster_selection_method\": 'eom',\n",
    "                    \"prediction_data\": True,\n",
    "                    #\"min_samples\": 15\n",
    "         },    \n",
    "        \"umap_args\": {\n",
    "                    \"n_neighbors\": 15,\n",
    "                    \"n_components\": 5,\n",
    "                    \"min_dist\": 0.0,\n",
    "                    \"metric\": 'cosine',\n",
    "                    \"low_memory\": False,\n",
    "                    \"random_state\": 42\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef95d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Maybe parallelism disable?\n",
    "#import os\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from src import preprocessor\n",
    "from src.utils import load_documents\n",
    "\n",
    "docs, labels = load_documents(dataset=configs['dataset'])\n",
    "\n",
    "if 'preprocessing_funcs' in configs:\n",
    "    docs = preprocessor.run(data=docs, prep_functions=configs['preprocessing_funcs'])\n",
    "    \n",
    "algorithm_args = configs['algorithm_args']\n",
    "algorithm_args.update(data_name=configs['dataset'],docs=docs,labels=labels)\n",
    "print(f'Running with {algorithm_args[\"number_topics\"]} topics')\n",
    "\n",
    "if configs['algorithm'] == 'bertopic':\n",
    "    # Encode data with embedding model\n",
    "    model = SentenceTransformer(algorithm_args['embedding_model'])\n",
    "    embeddings = model.encode(docs, show_progress_bar=True)\n",
    "    \n",
    "    trainer = Trainer(dataset = configs['dataset'],\n",
    "                      model_name = configs['algorithm'],\n",
    "                      params = algorithm_args,\n",
    "                      topk = algorithm_args[\"top_n_words\"],\n",
    "                      bt_embeddings = embeddings,\n",
    "                      )\n",
    "    \n",
    "    model, df_output_doc_topic, df_output_topic_word = trainer.train()\n",
    "    \n",
    "    from src.evaluator import compute_topic_scores\n",
    "    df_output_topic_word = compute_topic_scores(df_output_doc_topic, df_output_topic_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97fec096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>method</th>\n",
       "      <th>method_specific_params</th>\n",
       "      <th>dataset</th>\n",
       "      <th>num_given_topics</th>\n",
       "      <th>reduced</th>\n",
       "      <th>topic_num</th>\n",
       "      <th>topic_size</th>\n",
       "      <th>topic_words</th>\n",
       "      <th>word_scores</th>\n",
       "      <th>num_detected_topics</th>\n",
       "      <th>num_final_topics</th>\n",
       "      <th>duration_secs</th>\n",
       "      <th>diversity_unique</th>\n",
       "      <th>diversity_inv_rbo</th>\n",
       "      <th>coherence_npmi</th>\n",
       "      <th>coherence_v</th>\n",
       "      <th>rand_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1658309733</td>\n",
       "      <td>lda-bert</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'numbe...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>[smoke, debbie, cyclone, wildfire, earthquake,...</td>\n",
       "      <td>[0.03574329813160033, 0.03330625507717303, 0.0...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.295372</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.629645</td>\n",
       "      <td>-0.30072</td>\n",
       "      <td>0.316743</td>\n",
       "      <td>0.64121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1658309733</td>\n",
       "      <td>lda-bert</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'numbe...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>116</td>\n",
       "      <td>[flood, wildfire, cyclone, rain, debbie, smoke...</td>\n",
       "      <td>[0.052525252525252523, 0.01616161616161616, 0....</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.295372</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.629645</td>\n",
       "      <td>-0.30072</td>\n",
       "      <td>0.316743</td>\n",
       "      <td>0.64121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1658309733</td>\n",
       "      <td>lda-bert</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'numbe...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>131</td>\n",
       "      <td>[earthquake, flood, debbie, cyclone, felt, old...</td>\n",
       "      <td>[0.06908267270668177, 0.03737259343148358, 0.0...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.295372</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.629645</td>\n",
       "      <td>-0.30072</td>\n",
       "      <td>0.316743</td>\n",
       "      <td>0.64121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       run_id    method                             method_specific_params  \\\n",
       "0  1658309733  lda-bert  {'embedding_model': 'all-MiniLM-L6-v2', 'numbe...   \n",
       "1  1658309733  lda-bert  {'embedding_model': 'all-MiniLM-L6-v2', 'numbe...   \n",
       "2  1658309733  lda-bert  {'embedding_model': 'all-MiniLM-L6-v2', 'numbe...   \n",
       "\n",
       "      dataset  num_given_topics  reduced  topic_num  topic_size  \\\n",
       "0  crisis_toy                 3    False          0         149   \n",
       "1  crisis_toy                 3    False          1         116   \n",
       "2  crisis_toy                 3    False          2         131   \n",
       "\n",
       "                                         topic_words  \\\n",
       "0  [smoke, debbie, cyclone, wildfire, earthquake,...   \n",
       "1  [flood, wildfire, cyclone, rain, debbie, smoke...   \n",
       "2  [earthquake, flood, debbie, cyclone, felt, old...   \n",
       "\n",
       "                                         word_scores  num_detected_topics  \\\n",
       "0  [0.03574329813160033, 0.03330625507717303, 0.0...                    3   \n",
       "1  [0.052525252525252523, 0.01616161616161616, 0....                    3   \n",
       "2  [0.06908267270668177, 0.03737259343148358, 0.0...                    3   \n",
       "\n",
       "   num_final_topics  duration_secs  diversity_unique  diversity_inv_rbo  \\\n",
       "0                 3      10.295372          0.666667           0.629645   \n",
       "1                 3      10.295372          0.666667           0.629645   \n",
       "2                 3      10.295372          0.666667           0.629645   \n",
       "\n",
       "   coherence_npmi  coherence_v  rand_index  \n",
       "0        -0.30072     0.316743     0.64121  \n",
       "1        -0.30072     0.316743     0.64121  \n",
       "2        -0.30072     0.316743     0.64121  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output_topic_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c40a7",
   "metadata": {},
   "source": [
    "## Parametric Run: LDA-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03eb47b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'dataset': 'crisis_toy',\n",
    "    'preprocessing_funcs': [\n",
    "        'to_lowercase',\n",
    "        'standardize_accented_chars',\n",
    "        'remove_url',\n",
    "        'expand_contractions',\n",
    "        'expand_missing_delimiter',\n",
    "        'remove_mentions',\n",
    "        'remove_hashtags',\n",
    "        'remove_new_lines',\n",
    "        'keep_only_alphabet',\n",
    "        # 'remove_extra_spaces',\n",
    "        'remove_english_stop_words',\n",
    "        'lemmatize_noun',\n",
    "        'correct_typo'\n",
    "    ],\n",
    "    'algorithm': 'lda-bert',\n",
    "    'algorithm_args': {\n",
    "            'embedding_model': \"all-MiniLM-L6-v2\",\n",
    "            'number_topics': 3,\n",
    "            'top_n_words': 10,\n",
    "            'gamma': 15 ,\n",
    "            'random_state': 42\n",
    "    }     \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3b623da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Available Preprocessing Functions in the Module:['to_lowercase', 'standardize_accented_chars', 'remove_url', 'expand_missing_delimiter', 'remove_mentions', 'remove_hashtags', 'keep_only_alphabet', 'remove_new_lines', 'remove_extra_spaces', 'remove_html_tags', 'expand_contractions', 'remove_english_stop_words', 'lemmatize', 'lemmatize_verb', 'lemmatize_noun', 'lemmatize_adjective', 'correct_typo']\n",
      "[INFO] Preprocessing starting..\n",
      "[INFO] These string preprocessing methods will be applied to the data in order:\n",
      "(  'to_lowercase',\n",
      "   'standardize_accented_chars',\n",
      "   'remove_url',\n",
      "   'expand_missing_delimiter',\n",
      "   'remove_mentions',\n",
      "   'remove_hashtags',\n",
      "   'remove_new_lines',\n",
      "   'keep_only_alphabet')\n",
      "[INFO] Then, these tokenized preprocessing methods will be applied to the data in order:\n",
      "(  '__tokenize',\n",
      "   'expand_contractions',\n",
      "   'remove_english_stop_words',\n",
      "   'lemmatize_noun',\n",
      "   'correct_typo',\n",
      "   '__glue')\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[INFO] Preprocessing completed in 6.463 seconds..\n",
      "Running with 3 topics\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560bbe1f94dd491da8f76fd5b8ff43b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing raw texts ...\n",
      "Preprocessing raw texts. Done!\n",
      "Clustering embeddings ...\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "Getting vector representations for BERT ...\n",
      "Getting vector representations for BERT. Done!\n",
      "Fitting Autoencoder ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 11:35:37.211782: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-20 11:35:37.211835: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-20 11:35:37.211904: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LAPTOP-2D4JM72S): /proc/driver/nvidia/version does not exist\n",
      "2022-07-20 11:35:37.213146: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Autoencoder Done!\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "Clustering embeddings. Done!\n",
      "the words scores are: [[0.03574329813160033, 0.03330625507717303, 0.030056864337936636, 0.025995125913891144, 0.021121039805036556, 0.016246953696181964, 0.012997562956945572, 0.012997562956945572, 0.012185215272136474, 0.011372867587327376], [0.052525252525252523, 0.01616161616161616, 0.01616161616161616, 0.015151515151515152, 0.015151515151515152, 0.014141414141414142, 0.013131313131313131, 0.013131313131313131, 0.010101010101010102, 0.00808080808080808], [0.06908267270668177, 0.03737259343148358, 0.027180067950169876, 0.026047565118912798, 0.010192525481313703, 0.010192525481313703, 0.007927519818799546, 0.007927519818799546, 0.006795016987542469, 0.0056625141562853904]]\n",
      "and have length: 3\n"
     ]
    }
   ],
   "source": [
    "from src import preprocessor\n",
    "from src.utils import load_documents\n",
    "\n",
    "docs, labels = load_documents(dataset=configs['dataset'])\n",
    "\n",
    "if 'preprocessing_funcs' in configs:\n",
    "    docs = preprocessor.run(data=docs, prep_functions=configs['preprocessing_funcs'])\n",
    "    \n",
    "algorithm_args = configs['algorithm_args']\n",
    "algorithm_args.update(data_name=configs['dataset'],docs=docs,labels=labels)\n",
    "print(f'Running with {algorithm_args[\"number_topics\"]} topics')\n",
    "\n",
    "if configs['algorithm'] == 'lda-bert':\n",
    "    # Encode data with embedding model\n",
    "    model = SentenceTransformer(algorithm_args['embedding_model'])\n",
    "    embeddings = model.encode(docs, show_progress_bar=True)\n",
    "    \n",
    "    trainer = Trainer(dataset = configs['dataset'],\n",
    "                      model_name = configs['algorithm'],\n",
    "                      params = algorithm_args,\n",
    "                      topk = algorithm_args[\"top_n_words\"],\n",
    "                      bt_embeddings = embeddings,\n",
    "                      )\n",
    "    \n",
    "    model, df_output_doc_topic, df_output_topic_word = trainer.train()\n",
    "    \n",
    "    from src.evaluator import compute_topic_scores\n",
    "    df_output_topic_word = compute_topic_scores(df_output_doc_topic, df_output_topic_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dba49ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>method</th>\n",
       "      <th>method_specific_params</th>\n",
       "      <th>dataset</th>\n",
       "      <th>num_given_topics</th>\n",
       "      <th>reduced</th>\n",
       "      <th>topic_num</th>\n",
       "      <th>topic_size</th>\n",
       "      <th>topic_words</th>\n",
       "      <th>word_scores</th>\n",
       "      <th>num_detected_topics</th>\n",
       "      <th>num_final_topics</th>\n",
       "      <th>duration_secs</th>\n",
       "      <th>diversity_unique</th>\n",
       "      <th>diversity_inv_rbo</th>\n",
       "      <th>coherence_npmi</th>\n",
       "      <th>coherence_v</th>\n",
       "      <th>rand_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1658309733</td>\n",
       "      <td>lda-bert</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'numbe...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>[smoke, debbie, cyclone, wildfire, earthquake,...</td>\n",
       "      <td>[0.03574329813160033, 0.03330625507717303, 0.0...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.295372</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.629645</td>\n",
       "      <td>-0.30072</td>\n",
       "      <td>0.316743</td>\n",
       "      <td>0.64121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1658309733</td>\n",
       "      <td>lda-bert</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'numbe...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>116</td>\n",
       "      <td>[flood, wildfire, cyclone, rain, debbie, smoke...</td>\n",
       "      <td>[0.052525252525252523, 0.01616161616161616, 0....</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.295372</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.629645</td>\n",
       "      <td>-0.30072</td>\n",
       "      <td>0.316743</td>\n",
       "      <td>0.64121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1658309733</td>\n",
       "      <td>lda-bert</td>\n",
       "      <td>{'embedding_model': 'all-MiniLM-L6-v2', 'numbe...</td>\n",
       "      <td>crisis_toy</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>131</td>\n",
       "      <td>[earthquake, flood, debbie, cyclone, felt, old...</td>\n",
       "      <td>[0.06908267270668177, 0.03737259343148358, 0.0...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.295372</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.629645</td>\n",
       "      <td>-0.30072</td>\n",
       "      <td>0.316743</td>\n",
       "      <td>0.64121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       run_id    method                             method_specific_params  \\\n",
       "0  1658309733  lda-bert  {'embedding_model': 'all-MiniLM-L6-v2', 'numbe...   \n",
       "1  1658309733  lda-bert  {'embedding_model': 'all-MiniLM-L6-v2', 'numbe...   \n",
       "2  1658309733  lda-bert  {'embedding_model': 'all-MiniLM-L6-v2', 'numbe...   \n",
       "\n",
       "      dataset  num_given_topics  reduced  topic_num  topic_size  \\\n",
       "0  crisis_toy                 3    False          0         149   \n",
       "1  crisis_toy                 3    False          1         116   \n",
       "2  crisis_toy                 3    False          2         131   \n",
       "\n",
       "                                         topic_words  \\\n",
       "0  [smoke, debbie, cyclone, wildfire, earthquake,...   \n",
       "1  [flood, wildfire, cyclone, rain, debbie, smoke...   \n",
       "2  [earthquake, flood, debbie, cyclone, felt, old...   \n",
       "\n",
       "                                         word_scores  num_detected_topics  \\\n",
       "0  [0.03574329813160033, 0.03330625507717303, 0.0...                    3   \n",
       "1  [0.052525252525252523, 0.01616161616161616, 0....                    3   \n",
       "2  [0.06908267270668177, 0.03737259343148358, 0.0...                    3   \n",
       "\n",
       "   num_final_topics  duration_secs  diversity_unique  diversity_inv_rbo  \\\n",
       "0                 3      10.295372          0.666667           0.629645   \n",
       "1                 3      10.295372          0.666667           0.629645   \n",
       "2                 3      10.295372          0.666667           0.629645   \n",
       "\n",
       "   coherence_npmi  coherence_v  rand_index  \n",
       "0        -0.30072     0.316743     0.64121  \n",
       "1        -0.30072     0.316743     0.64121  \n",
       "2        -0.30072     0.316743     0.64121  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output_topic_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fb2c569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>Document ID</th>\n",
       "      <th>Document</th>\n",
       "      <th>Real Label</th>\n",
       "      <th>Assigned Topic Num</th>\n",
       "      <th>Assignment Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>0</td>\n",
       "      <td>thereformedcrow nah going go earthquake</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>1</td>\n",
       "      <td>think earthquake</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>2</td>\n",
       "      <td>uhh else felt earthquake though</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>3</td>\n",
       "      <td>bay area nice size earthquake</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>4</td>\n",
       "      <td>thought dad farting turn earthquake</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>391</td>\n",
       "      <td>people keep asking good safe live cyclone debb...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>392</td>\n",
       "      <td>ayyeeee work got cancelled flood thank cyclone...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>393</td>\n",
       "      <td>jetstarairways helpful need change flight due ...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>394</td>\n",
       "      <td>getting hit ex tropical cyclone named debbie r...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1658231276</td>\n",
       "      <td>395</td>\n",
       "      <td>wet wet wet today great day stay indoors ex tr...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>396 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         run_id  Document ID  \\\n",
       "0    1658231276            0   \n",
       "1    1658231276            1   \n",
       "2    1658231276            2   \n",
       "3    1658231276            3   \n",
       "4    1658231276            4   \n",
       "..          ...          ...   \n",
       "391  1658231276          391   \n",
       "392  1658231276          392   \n",
       "393  1658231276          393   \n",
       "394  1658231276          394   \n",
       "395  1658231276          395   \n",
       "\n",
       "                                              Document  Real Label  \\\n",
       "0              thereformedcrow nah going go earthquake  earthquake   \n",
       "1                                     think earthquake  earthquake   \n",
       "2                      uhh else felt earthquake though  earthquake   \n",
       "3                        bay area nice size earthquake  earthquake   \n",
       "4                  thought dad farting turn earthquake  earthquake   \n",
       "..                                                 ...         ...   \n",
       "391  people keep asking good safe live cyclone debb...   hurricane   \n",
       "392  ayyeeee work got cancelled flood thank cyclone...   hurricane   \n",
       "393  jetstarairways helpful need change flight due ...   hurricane   \n",
       "394  getting hit ex tropical cyclone named debbie r...   hurricane   \n",
       "395  wet wet wet today great day stay indoors ex tr...   hurricane   \n",
       "\n",
       "     Assigned Topic Num  Assignment Score  \n",
       "0                     2                 1  \n",
       "1                     2                 1  \n",
       "2                     2                 1  \n",
       "3                     0                 1  \n",
       "4                     2                 1  \n",
       "..                  ...               ...  \n",
       "391                   0                 1  \n",
       "392                   1                 1  \n",
       "393                   0                 1  \n",
       "394                   0                 1  \n",
       "395                   0                 1  \n",
       "\n",
       "[396 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output_doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6eee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
