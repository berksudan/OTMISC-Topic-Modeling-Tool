run_id	method	method_specific_params	dataset	num_given_topics	reduced	topic_num	topic_size	topic_words	word_scores	num_detected_topics	num_final_topics	duration_secs	diversity_unique	diversity_inv_rbo	coherence_npmi	coherence_v	rand_index
1658738190	bertopic	{'embedding_model': 'all-mpnet-base-v2', 'top_n_words': 10, 'n_gram_range_tuple': (1, 1), 'min_docs_per_topic': 15, 'num_topics': 10, 'cluster_model': 'hdbscan', 'hdbscan_args': {'min_cluster_size': 10, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True}, 'umap_args': {'n_neighbors': 15, 'n_components': 5, 'min_dist': 0.0, 'metric': 'cosine', 'low_memory': False, 'random_state': 42}}	yahoo	10	True	-1	51847	['the', 'to', 'and', 'of', 'you', 'is', 'in', 'it', 'that', 'for']	[0.05207553448580612, 0.041395744636553156, 0.03890738709734219, 0.0366753474187071, 0.03395645480476769, 0.032490146567684945, 0.03014632529583004, 0.027221108366710565, 0.02693946719839962, 0.02285323883281727]	723	11	117.82278394699097	0.43	0.725390658454127	0.10279396003350576	0.4853444595764759	0.3038940060112113
1658738190	bertopic	{'embedding_model': 'all-mpnet-base-v2', 'top_n_words': 10, 'n_gram_range_tuple': (1, 1), 'min_docs_per_topic': 15, 'num_topics': 10, 'cluster_model': 'hdbscan', 'hdbscan_args': {'min_cluster_size': 10, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True}, 'umap_args': {'n_neighbors': 15, 'n_components': 5, 'min_dist': 0.0, 'metric': 'cosine', 'low_memory': False, 'random_state': 42}}	yahoo	10	True	0	1191	['her', 'she', 'you', 'to', 'and', 'that', 'is', 'if', 'be', 'your']	[0.09545221256934643, 0.07786792592907929, 0.06556124676846757, 0.047971130834197885, 0.042824014630707834, 0.0360903035909474, 0.030604422744228595, 0.030060040798365908, 0.030010326774277035, 0.0289448722718897]	723	11	117.82278394699097	0.43	0.725390658454127	0.10279396003350576	0.4853444595764759	0.3038940060112113
1658738190	bertopic	{'embedding_model': 'all-mpnet-base-v2', 'top_n_words': 10, 'n_gram_range_tuple': (1, 1), 'min_docs_per_topic': 15, 'num_topics': 10, 'cluster_model': 'hdbscan', 'hdbscan_args': {'min_cluster_size': 10, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True}, 'umap_args': {'n_neighbors': 15, 'n_components': 5, 'min_dist': 0.0, 'metric': 'cosine', 'low_memory': False, 'random_state': 42}}	yahoo	10	True	1	1190	['the', 'my', 'me', 'you', 'to', 'and', 'song', 'it', 'in', 'of']	[0.04756924573732121, 0.040548499741345945, 0.03587483495699038, 0.0344092509692839, 0.031366938974343936, 0.02988979841433125, 0.028762147015557016, 0.02750918723894171, 0.026914233743334237, 0.026553119949183603]	723	11	117.82278394699097	0.43	0.725390658454127	0.10279396003350576	0.4853444595764759	0.3038940060112113
1658738190	bertopic	{'embedding_model': 'all-mpnet-base-v2', 'top_n_words': 10, 'n_gram_range_tuple': (1, 1), 'min_docs_per_topic': 15, 'num_topics': 10, 'cluster_model': 'hdbscan', 'hdbscan_args': {'min_cluster_size': 10, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True}, 'umap_args': {'n_neighbors': 15, 'n_components': 5, 'min_dist': 0.0, 'metric': 'cosine', 'low_memory': False, 'random_state': 42}}	yahoo	10	True	2	1036	['', '', '', '', '', '', '', '', '', '']	[1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 1e-05]	723	11	117.82278394699097	0.43	0.725390658454127	0.10279396003350576	0.4853444595764759	0.3038940060112113
1658738190	bertopic	{'embedding_model': 'all-mpnet-base-v2', 'top_n_words': 10, 'n_gram_range_tuple': (1, 1), 'min_docs_per_topic': 15, 'num_topics': 10, 'cluster_model': 'hdbscan', 'hdbscan_args': {'min_cluster_size': 10, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True}, 'umap_args': {'n_neighbors': 15, 'n_components': 5, 'min_dist': 0.0, 'metric': 'cosine', 'low_memory': False, 'random_state': 42}}	yahoo	10	True	3	867	['the', 'you', 'to', 'and', 'it', 'your', 'windows', 'click', 'on', 'is']	[0.05435432029141634, 0.04892494912025526, 0.044782985970617195, 0.04340785374391316, 0.03847317290372108, 0.03813117111244475, 0.035336888433004754, 0.03407144830226412, 0.03286568461092747, 0.027319667615495605]	723	11	117.82278394699097	0.43	0.725390658454127	0.10279396003350576	0.4853444595764759	0.3038940060112113
1658738190	bertopic	{'embedding_model': 'all-mpnet-base-v2', 'top_n_words': 10, 'n_gram_range_tuple': (1, 1), 'min_docs_per_topic': 15, 'num_topics': 10, 'cluster_model': 'hdbscan', 'hdbscan_args': {'min_cluster_size': 10, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True}, 'umap_args': {'n_neighbors': 15, 'n_components': 5, 'min_dist': 0.0, 'metric': 'cosine', 'low_memory': False, 'random_state': 42}}	yahoo	10	True	4	745	['you', 'and', 'your', 'to', 'weight', 'eat', 'the', 'fat', 'of', 'is']	[0.054899398209813446, 0.04582472038385498, 0.0418999574331484, 0.03920306227974571, 0.03702730914322846, 0.03552398693734366, 0.03206506077246978, 0.030945936778047283, 0.030928208195148178, 0.028304950793282175]	723	11	117.82278394699097	0.43	0.725390658454127	0.10279396003350576	0.4853444595764759	0.3038940060112113
1658738190	bertopic	{'embedding_model': 'all-mpnet-base-v2', 'top_n_words': 10, 'n_gram_range_tuple': (1, 1), 'min_docs_per_topic': 15, 'num_topics': 10, 'cluster_model': 'hdbscan', 'hdbscan_args': {'min_cluster_size': 10, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True}, 'umap_args': {'n_neighbors': 15, 'n_components': 5, 'min_dist': 0.0, 'metric': 'cosine', 'low_memory': False, 'random_state': 42}}	yahoo	10	True	5	712	['you', 'he', 'him', 'to', 'and', 'if', 'it', 'that', 'is', 'the']	[0.0771641933572137, 0.07224979998901578, 0.07202881504506375, 0.052306999369579624, 0.043035202863915274, 0.03935819464896446, 0.031542242297555, 0.030623165391102804, 0.029573425132589638, 0.029425051217729075]	723	11	117.82278394699097	0.43	0.725390658454127	0.10279396003350576	0.4853444595764759	0.3038940060112113
1658738190	bertopic	{'embedding_model': 'all-mpnet-base-v2', 'top_n_words': 10, 'n_gram_range_tuple': (1, 1), 'min_docs_per_topic': 15, 'num_topics': 10, 'cluster_model': 'hdbscan', 'hdbscan_args': {'min_cluster_size': 10, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True}, 'umap_args': {'n_neighbors': 15, 'n_components': 5, 'min_dist': 0.0, 'metric': 'cosine', 'low_memory': False, 'random_state': 42}}	yahoo	10	True	6	698	['the', 'and', 'it', 'to', 'of', 'you', 'your', 'is', 'hair', 'skin']	[0.04603753880367255, 0.043981632336755686, 0.04314899082441526, 0.037081064641312174, 0.03439377926398003, 0.03200025465499655, 0.03073490051624011, 0.030721968217849935, 0.02839036538787137, 0.024857962461209165]	723	11	117.82278394699097	0.43	0.725390658454127	0.10279396003350576	0.4853444595764759	0.3038940060112113
1658738190	bertopic	{'embedding_model': 'all-mpnet-base-v2', 'top_n_words': 10, 'n_gram_range_tuple': (1, 1), 'min_docs_per_topic': 15, 'num_topics': 10, 'cluster_model': 'hdbscan', 'hdbscan_args': {'min_cluster_size': 10, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True}, 'umap_args': {'n_neighbors': 15, 'n_components': 5, 'min_dist': 0.0, 'metric': 'cosine', 'low_memory': False, 'random_state': 42}}	yahoo	10	True	7	588	['the', 'cup', 'in', 'world', 'they', 'and', 'team', 'brazil', 'to', 'will']	[0.056744876506618545, 0.0435591107638316, 0.039254059821364144, 0.03862206845107794, 0.035328603584075385, 0.034917004026114445, 0.033391315093159415, 0.03249369361429214, 0.029896668117931628, 0.028695965861691937]	723	11	117.82278394699097	0.43	0.725390658454127	0.10279396003350576	0.4853444595764759	0.3038940060112113
1658738190	bertopic	{'embedding_model': 'all-mpnet-base-v2', 'top_n_words': 10, 'n_gram_range_tuple': (1, 1), 'min_docs_per_topic': 15, 'num_topics': 10, 'cluster_model': 'hdbscan', 'hdbscan_args': {'min_cluster_size': 10, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True}, 'umap_args': {'n_neighbors': 15, 'n_components': 5, 'min_dist': 0.0, 'metric': 'cosine', 'low_memory': False, 'random_state': 42}}	yahoo	10	True	8	576	['yes', 'no', 'nope', 'it', 'do', 'not', 'you', 'is', 'they', 'can']	[0.35107778862357886, 0.25357628906073, 0.07374057864768536, 0.06486768967072147, 0.05318176298128087, 0.04614544005166161, 0.04539620401288099, 0.043194688248021626, 0.038570299199410615, 0.03433906848611559]	723	11	117.82278394699097	0.43	0.725390658454127	0.10279396003350576	0.4853444595764759	0.3038940060112113
1658738190	bertopic	{'embedding_model': 'all-mpnet-base-v2', 'top_n_words': 10, 'n_gram_range_tuple': (1, 1), 'min_docs_per_topic': 15, 'num_topics': 10, 'cluster_model': 'hdbscan', 'hdbscan_args': {'min_cluster_size': 10, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True}, 'umap_args': {'n_neighbors': 15, 'n_components': 5, 'min_dist': 0.0, 'metric': 'cosine', 'low_memory': False, 'random_state': 42}}	yahoo	10	True	9	550	['the', 'of', 'and', 'to', 'in', 'that', 'war', 'iraq', 'is', 'we']	[0.06423721285556595, 0.04354302793038857, 0.043240326616936935, 0.042288317534369374, 0.03421910953002154, 0.03061416445946357, 0.029252904579348323, 0.028493953306219337, 0.02832881003424598, 0.025459831095003365]	723	11	117.82278394699097	0.43	0.725390658454127	0.10279396003350576	0.4853444595764759	0.3038940060112113
