% Use this as your main bibliography file.

@article{lda,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={Journal of machine Learning research},
  volume={3},
  number={Jan},
  pages={993--1022},
  year={2003}
}

@article{nmf,
  title={Algorithms for non-negative matrix factorization},
  author={Lee, Daniel and Seung, H Sebastian},
  journal={Advances in neural information processing systems},
  volume={13},
  year={2000}
}

@misc{bertopic,
  doi = {10.48550/ARXIV.2203.05794},
  
  url = {https://arxiv.org/abs/2203.05794},
  
  author = {Grootendorst, Maarten},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BERTopic: Neural topic modeling with a class-based TF-IDF procedure},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{top2vec,
  doi = {10.48550/ARXIV.2008.09470},
  
  url = {https://arxiv.org/abs/2008.09470},
  
  author = {Angelov, Dimo},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Top2Vec: Distributed Representations of Topics},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{ctm,
    title = "Cross-lingual Contextualized Topic Models with Zero-shot Learning",
    author = "Bianchi, Federico  and
      Terragni, Silvia  and
      Hovy, Dirk  and
      Nozza, Debora  and
      Fersini, Elisabetta",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.143",
    doi = "10.18653/v1/2021.eacl-main.143",
    pages = "1676--1683",
    abstract = "Many data sets (e.g., reviews, forums, news, etc.) exist parallelly in multiple languages. They all cover the same content, but the linguistic differences make it impossible to use traditional, bag-of-word-based topic models. Models have to be either single-language or suffer from a huge, but extremely sparse vocabulary. Both issues can be addressed by transfer learning. In this paper, we introduce a zero-shot cross-lingual topic model. Our model learns topics on one language (here, English), and predicts them for unseen documents in different languages (here, Italian, French, German, and Portuguese). We evaluate the quality of the topic predictions for the same document in different languages. Our results show that the transferred topics are coherent and stable across languages, which suggests exciting future research directions.",
}

@InProceedings{lda-bert,
author="Basmatkar, Pranjali
and Maurya, Mahesh",
editor="Bindhu, V.
and Tavares, Jo{\~a}o Manuel R. S.
and Du, Ke-Lin",
title="An Overview of Contextual Topic Modeling Using Bidirectional Encoder Representations from Transformers",
booktitle="Proceedings of Third International Conference on Communication, Computing and Electronics Systems ",
year="2022",
publisher="Springer Singapore",
address="Singapore",
pages="489--504",
abstract="Topic modeling refers to a range of algorithms in natural language processing that gives us an insight into the `latent' semantic topics or patterns in a collection of documents. These patterns of word co-occurrence are used to determine the hidden `topics' which are present in the corpus. Topic modeling has been used successfully for information retrieval, classifying documents, summarizing them and for exploratory analysis of large corpora of texts. This survey studies various algorithms that have been used for topic modeling over time including TF-IDF, latent Dirichlet algorithm (LDA), clustering on sentence-level BERT embeddings and a newer hybrid approach of generating contextual topics using a combination of LDA and BERT vectors. This survey will analyze the advantages and limitations of these algorithms.",
isbn="978-981-16-8862-1"
}

@misc{sbert,
  doi = {https://doi.org/10.48550/arxiv.1908.10084},
  
  url = {https://arxiv.org/abs/1908.10084},
  
  author = {Reimers, Nils and Gurevych, Iryna},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}


@misc{incoherence,
  doi = {10.48550/ARXIV.2107.02173},
  
  url = {https://arxiv.org/abs/2107.02173},
  
  author = {Hoyle, Alexander and Goel, Pranav and Peskov, Denis and Hian-Cheong, Andrew and Boyd-Graber, Jordan and Resnik, Philip},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@ARTICLE{tm_comparison,
  
AUTHOR={Egger, Roman and Yu, Joanne},   
	 
TITLE={A Topic Modeling Comparison Between LDA, NMF, Top2Vec, and BERTopic to Demystify Twitter Posts},      
	
JOURNAL={Frontiers in Sociology},      
	
VOLUME={7},           
	
YEAR={2022},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fsoc.2022.886498},       
	
DOI={10.3389/fsoc.2022.886498},      
	
ISSN={2297-7775},   
   
ABSTRACT={The richness of social media data has opened a new avenue for social science research to gain insights into human behaviors and experiences. In particular, emerging data-driven approaches relying on topic models provide entirely new perspectives on interpreting social phenomena. However, the short, text-heavy, and unstructured nature of social media content often leads to methodological challenges in both data collection and analysis. In order to bridge the developing field of computational science and empirical social research, this study aims to evaluate the performance of four topic modeling techniques; namely latent Dirichlet allocation (LDA), non-negative matrix factorization (NMF), Top2Vec, and BERTopic. In view of the interplay between human relations and digital media, this research takes Twitter posts as the reference point and assesses the performance of different algorithms concerning their strengths and weaknesses in a social science context. Based on certain details during the analytical procedures and on quality issues, this research sheds light on the efficacy of using BERTopic and NMF to analyze Twitter data.}
}

@INPROCEEDINGS{lda-bert_survey,
  author={Sethia, Kashi and Saxena, Madhur and Goyal, Mukul and Yadav, R.K.},
  booktitle={2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)}, 
  title={Framework for Topic Modeling using BERT, LDA and K-Means}, 
  year={2022},
  volume={},
  number={},
  pages={2204-2208},
  doi={10.1109/ICACITE53722.2022.9823442}
}

@inproceedings{coherence_survey,
author = {R\"{o}der, Michael and Both, Andreas and Hinneburg, Alexander},
title = {Exploring the Space of Topic Coherence Measures},
year = {2015},
isbn = {9781450333177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684822.2685324},
doi = {10.1145/2684822.2685324},
abstract = {Quantifying the coherence of a set of statements is a long standing problem with many potential applications that has attracted researchers from different sciences. The special case of measuring coherence of topics has been recently studied to remedy the problem that topic models give no guaranty on the interpretablity of their output. Several benchmark datasets were produced that record human judgements of the interpretability of topics. We are the first to propose a framework that allows to construct existing word based coherence measures as well as new ones by combining elementary components. We conduct a systematic search of the space of coherence measures using all publicly available topic relevance data for the evaluation. Our results show that new combinations of components outperform existing measures with respect to correlation to human ratings. nFinally, we outline how our results can be transferred to further applications in the context of text mining, information retrieval and the world wide web.},
booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining},
pages = {399â€“408},
numpages = {10},
keywords = {topic coherence, topic evaluation, topic model},
location = {Shanghai, China},
series = {WSDM '15}
}

@inproceedings{octis,
    title = "{OCTIS}: Comparing and Optimizing Topic models is Simple!",
    author = "Terragni, Silvia  and
      Fersini, Elisabetta  and
      Galuzzi, Bruno Giovanni  and
      Tropeano, Pietro  and
      Candelieri, Antonio",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-demos.31",
    doi = "10.18653/v1/2021.eacl-demos.31",
    pages = "263--270",
    abstract = "In this paper, we present OCTIS, a framework for training, analyzing, and comparing Topic Models, whose optimal hyper-parameters are estimated using a Bayesian Optimization approach. The proposed solution integrates several state-of-the-art topic models and evaluation metrics. These metrics can be targeted as objective by the underlying optimization procedure to determine the best hyper-parameter configuration. OCTIS allows researchers and practitioners to have a fair comparison between topic models of interest, using several benchmark datasets and well-known evaluation metrics, to integrate novel algorithms, and to have an interactive visualization of the results for understanding the behavior of each model. The code is available at the following link: https://github.com/MIND-Lab/OCTIS.",
}

@inproceedings{hdbscan,
	doi = {10.1109/mfi49285.2020.9235263},
  
	url = {https://doi.org/10.1109/mfi49285.2020.9235263},
  
	year = 2020,
	month = {sep},
  
	publisher = {{IEEE}},
  
	author = {Claudia Malzer and Marcus Baum},
  
	title = {A Hybrid Approach To Hierarchical Density-based Cluster Selection},
  
	booktitle = {2020 {IEEE} International Conference on Multisensor Fusion and Integration for Intelligent Systems ({MFI})}
}

@misc{umap,
  doi = {10.48550/ARXIV.1802.03426},
  
  url = {https://arxiv.org/abs/1802.03426},
  
  author = {McInnes, Leland and Healy, John and Melville, James},
  
  keywords = {Machine Learning (stat.ML), Computational Geometry (cs.CG), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
